[{"title":"PYNQ教程二:使用AXI总线实现PS与PL交互","path":"/2024/06/15/PYNQ教程二：PS与PL交互/","content":"ZYNQ架构分为处理器系统（PS）和可编程逻辑（PL）两大部分，PS运行在ARM Cortex A9处理器上，PL相当于FPGA，本文介绍如何使用AXI总线实现PS与PL之间的交互。 1 PL部分设计PL设计可以使用Vivado HLS开发，或者Verilog开发，这里介绍使用Vivado HLS的方式。 设计一个简单的模块，代码如下： 123456789void test(float input[5],float num,float output[5])&#123;#pragma HLS INTERFACE s_axilite port=return#pragma HLS INTERFACE s_axilite port=num#pragma HLS INTERFACE m_axi depth=5 port=input offset=slave bundle=INPUT#pragma HLS INTERFACE m_axi depth=5 port=output offset=slave bundle=OUTPUT\tfor(int i=0;i&lt;5;i++) output[i] = input[i] * num;&#125; 函数功能很简单，就是将输入数组乘上某一个数后交给输出数组。 pragma解释： “port&#x3D;return”表示配置块级协议，”s_axilite”表示将ap_start等信号放置于AXI从接口寄存器中。 其余几个都是配置端口协议，”m_axi”即配置为AXI主接口，”s_axilite”即配置为AXI从接口。因为主接口访问数据时需要知道数据所在的地址，”offset&#x3D;slave”表示将地址放在AXI从接口寄存器中。”bundle”可以将接口合并，这里bundle各不相同，也就意味着生成两个AXI主接口。 上面就是HLS生成的IP核。在HLS中打开solution1&#x2F;impl&#x2F;ip&#x2F;drivers&#x2F;*_v1_0&#x2F;src，找到一个以hw.h结尾的文件，打开后可以看到AXI从接口寄存器信息（也可以使用SDK的方式，但是我觉得这样更方便）。 2 Vivado Block Design 可以看到，ZYNQ有两个AXI_GP主接口、两个AXI_GP从接口、四个AXI_HP从接口和一个AXI_ACP从接口，这些接口的区别可以参考下面的视频： AXI接口讲解https://www.bilibili.com/video/BV1FD4y1E79q/?spm_id_from=333.999.0.0 在这里，由于HLS生成的IP核有一个AXI从接口和两个AXI主接口，因此这里ZYNQ至少使用一个AXI主接口和一个AXI从接口，然后使用AXI Interconnect或AXI SmartConnect实现多主多从之间的连接。这里可以使用Vivado的自动连接功能。 上图是一种连接方式，也可以使用AXI_GP从接口，不一定要用AXI_HP从接口。 3 PYNQ PS端代码PS主接口和PL从接口： 根据pynq.io官方的描述：”In an overlay, peripherals connected to the AXI General Purpose ports will have their registers or address space mapped into the system memory map. With PYNQ, the register, or address space of an IP can be accessed from Python using the MMIO class.” 原网址如下： MMIOhttps://pynq.readthedocs.io/en/latest/pynq_libraries/mmio.html 翻译过来就是：对于连接至PYNQ AXI_GP主接口的IP，PS可以在Python代码中使用MMIO类访问它们。 PS从接口和PS主接口： 同样根据官方的说法：”IP connected to the AXI Master (HP or ACP ports) has access to PS DRAM. Before IP in the PL accesses DRAM, some memory must first be allocated (reserved) for the IP to use and the size, and address of the memory passed to the IP. An array in Python, or Numpy, will be allocated somewhere in virtual memory. The physical memory address of the allocated memory must be provided to IP in the PL. “（这里第一句话感觉应该不是AXI Master而是AXI Slave才对） 原网址如下： Allocatehttps://pynq.readthedocs.io/en/latest/pynq_libraries/allocate.html 翻译过来就是：连接至PYNQ从接口的IP可以访问DDR内存（DRAM），不过PS需要预先分配出一段内存，并将这段内存的物理地址传递给PL的IP核。 接下来开始代码实现过程： （1）烧写底层设计的bit流文件： 1234from pynq import Overlayol = Overlay(&#x27;ps_pl.bit&#x27;)ol? ”ol?“可以查看overlay的信息： 可以看到，识别到hls生成的IP核”test_0“，该IP核被封装为一个MMIO类对象，可以通过”ol.test_0?”查看其详细信息，mmio类内部有”write”和”read”函数实现读写功能。 （2）使用allocate函数分配一段内存，该内存可以当作一个numpy数组来使用，同时device_address属性为其物理地址： 1234567from pynq import allocateimport numpy as npinput_buffer = allocate(shape=(5,),dtype=np.float32)input_buffer.flush()output_buffer = allocate(shape=(5,),dtype=np.float32)output_buffer.flush() （3）将内存地址传递给PL的IP核： 123test_ip = ol.test_0test_ip.write(0x10,input_buffer.device_address)test_ip.write(0x20,output_buffer.device_address) 前面hw.h文件显示的AXI从接口寄存器地址中，0x10和0x20分别是输入和输出的地址，因此这里将PS分配的内存的物理地址分别写入到这两个地方中。 （4）写入输入数据： 123456inputs = np.array([1,2,3,4,5],dtype=np.float32)np.copyto(input_buffer,inputs)import structnum = struct.unpack(&#x27;&lt;I&#x27;,struct.pack(&#x27;&lt;f&#x27;,3))[0]test_ip.write(0x18,num) 这里num不能直接写入3，因为HLS定义了num是float类型。但是也不能写入3.0，否则会报下面的错误： ValueError:&nbsp;Data&nbsp;type&nbsp;must&nbsp;be&nbsp;int&nbsp;or&nbsp;bytes. 因为MMIO类的”write”函数不能写入float类型数据。所以这里使用struct做了一个变换，从float类型转换成int类型，struct的用法可参考： struct函数解析https://zhuanlan.zhihu.com/p/387421751 （5）启动IP核工作，等待其工作结束： 12345import timetest_ip.write(0x0,1)while(test_ip.read(0x0)&amp;0x2 == 0): time.sleep(0.001) 0x0位置是控制信号，写入1则ap_start置1，IP核开始工作。而第2位为ap_done，因此这里采用轮询的方式，等待第2位为1则IP核工作结束。 （6）获取输出结果： 1234outputs = np.zeros([5])np.copyto(outputs,output_buffer)print(outputs) 结果如下：","tags":["PYNQ"],"categories":["PYNQ教程"]},{"title":"PYNQ教程一:GPIO的使用","path":"/2024/06/09/PYNQ教程一：GPIO的使用/","content":"GPIO通常用于读写寄存器，与外部设备的交互等场景。本文从PS GPIO和AXI GPIO两方面分别介绍它们的配置方法，以及如何使用PYNQ编写Python代码实现数据交互。 1 PS GPIO1.1 PS GPIO的配置PS GPIO分为MIO和EMIO，其中MIO共54bit，EMIO共64bit。 其使用方法可以参考以下的网站： pynq官方文档https://pynq.readthedocs.io/en/latest/pynq_package/pynq.gpio.html pynq.io网站上文档非常全面介绍了Pynq的各种函数，后面的AXI_GPIO和Arduino_IO都可以在这里找到相关的函数解析 MIO和EMIO的使用https://blog.csdn.net/qq_35712169/article/details/106038000 总的来说，MIO无需配置，其位置已经固定，可以将其配置为GPIO口，但是需要注意的是其与其它接口复用（比如USB、ETH等），可能存在冲突导致无法使用的情况，而且位置固定后反而不方便使用，所以我个人更推荐使用EMIO的方式。 EMIO没有固定的引脚，其配置方法较简单，打开ZYNQ7 IP核配置页面，然后按下面的路径打开： 打开后拉到最下面就可以看到GPIO的配置，这里可以选择使用的EMIO的位数： 配置完后可以看到IP核多了一个GPIO_0接口，然后可以将这些端口绑定到特定的引脚上。 为了方便测试读写功能，可以将数据传输给其它的IP核，并获取其输出。这里可以使用“Utility Vector Logic”构建一个与门，输入输出大小为2。 GPIO_O只有一个，而与门有两个输入，因此这里需要通过两个“slice”IP核将64位数据分割开来，slice0配置如下，取输入的0-1位输出。 slice1配置类似，取输入2-3位输出。而后将这两个slice的输出连接到与门。 与门的输出并不能直接与GPIO_I连接。前面提到，EMIO只有64位，这里GPIO_O和GPIO_I显然是共用的，如果将与门的输出连接到GPIO_I，那么其结果将会发给低位，也就是0-1位，但是前面0-1位已经被GPIO_O用于输出了，当然如果0-1位配置为“inout”，那就没问题。但是后续的Python代码中PS GPIO的某一位只能被配置为“in”或“out”，而不是“inout”。因此这里需要加一个偏移项，将与门的输出传给GPIO_I的4-5位，从而避免该冲突，这可以通过“Constant”和“Concat”IP核实现偏移功能。 如图所示，“Constant”设置了4位，值为0，通过“Concat“将两者拼接起来，组成6位数据，与门的输出在最高两位。 在Synthesis中，会出现以下警告，但是无关紧要，如果不想出现这样的警告，可以将EMIO的位数设置为6位。 1.2 PS GPIO Python代码 首先烧写bit流文件，然后导入GPIO库，配置0-3位为输出，4-5位为输入，通过”read“和”write“函数实现GPIO的读写功能。 前面的内容参考了下面的视频，所以不太理解的也可以查看下面的视频讲解 youtube视频讲解https://youtu.be/a5NnLozPEI0?si=TiMc5d3H6u_wqcK1 1.3 Arduino的GPIO 使用Arduino的IO时，同样不需要配置，Python代码也比较简单，可以参考以下网站： Arduino IOhttps://rkblog.dev/posts/python/using-gpio-usb-and-hdmi-pynq-z2-board/ 2 AXI GPIO2.1 AXI GPIO配置AXI GPIO的配置相比于EMIO来说要简单一点，且可以同时使用多个AXI GPIO，灵活性很强。 IP核配置可以参考以下文章： AXI GPIO IP核配置https://blog.csdn.net/u011565038/article/details/138836478 我通常启用双通道，一个All Outputs，一个All Inputs，这样就不需要管Default Tri State Value这一栏了。 连接也很简单，先手动将AXI GPIO的输出与输入和非门连接，接下来使用Vivado的自动连接就行了。 2.2 AXI GPIO Python代码 首先烧写bit流文件，获取overlay信息： 得到AXI GPIO IP核的名字后，即可使用AxiGPIO函数将该IP核实例化为一个类对象，而后根据Pynq提供的函数实现读写功能。 也可以参考下面的官方例程： AXI GPIO官方案例https://github.com/Xilinx/PYNQ_Workshop/blob/master/Session_4/2_axi_gpio.ipynb 写在最后可以看到，文中引用了很多其它网站，毕竟在最初的学习中，也是广泛搜索了各种网站，然后才慢慢明白。所以我精选出这些我觉得有用的网站，可以省去广泛查找的过程，同时也更加全面。当然，有一定基础的看一下我写的也大概明白了。虽然不是很难，但是硬件的东西，别人说一下可能几分钟就知道了，如果自己去找资料看文档，可能要花费几个小时甚至于几天的时间。","tags":["PYNQ"],"categories":["PYNQ教程"]},{"title":"LSTM反向传播推导及C++代码实现","path":"/2024/06/02/LSTM反向传播推导及代码实现/","content":"之前在找资料时候发现大多数文章都只讲了LSTM前向传播，比较少有关于反向传播的内容，而且反向传播有很多公式，需要一步步地推才容易理解，另外在实现过程中还有一些小细节需要注意。 1 LSTM基础概念时间步（time_steps)：可以认为是输入的次数，有多少次输入就有“多少个”LSTM单元，但从本质上来看却是一个LSTM单元的复用。 如同简单RNN一样： 只有一个单元，但为了分析起来简单，所以通常将它展开，展开后看似有多个单元，但其实这些单元内部的权重完全一样，只不过中间状态和输入不一样。 输入维度(input_size)：每个单元接收的输入shape是1×input_size。 隐藏层大小&#x2F;输出维度（hidden_size&#x2F;output_size)：隐藏层大小决定了ht和Ct的大小，由于输出和ht相等，所以也决定了输出的大小。 例：假如存在一个任务，需要通过前面10个时刻的值来预测第11个时刻的值，此时time_steps为10，input_size为1（每次输入1个值，输入10次）。 2 LSTM权重大小计算前面提到，本质上只有一个LSTM单元，因此权重大小和time_steps无关，只与input_size和hidden_size有关。 LSTM单元需要计算三个门+一个候补细胞状态，每次都是分别将输入(1×input_size)和ht(1×hidden_size)与对应的权重进行矩阵相乘，输出大小都为1×hidden_size，最后加上偏置，因此可以推得权重大小为： $$ P&#x3D;4\\times(inputsize\\times hiddensize+hiddensize\\times hiddensize+hiddensize) $$ 3 LSTM反向传播推导3.1 最后一个LSTM单元LSTM最后一个单元其ht的梯度只来自输出。 3.1.1 输出门ot的反向传播由于输出门计算公式为： $$h_t&#x3D;o_t\\cdot\\tanh(C_t) \\tag{1}$$ 因此求输出门ot的梯度时，只需要将ht的梯度与tanh(Ct)相乘（点乘），这里tanh(Ct)的结果在前向传播时计算得到，因此需要在前向传播时将其保存下来： $$\\frac{\\partial L}{\\partial o_t}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)$$ 接着求输出门ot内部的梯度，由于ot门计算公式为： $$o_t&#x3D;\\sigma(x_t\\times W_{ox}+h_{t-1}\\times W_{oh}+b_o)$$ 所以求ot门内部的梯度，即可得到bo的梯度，由于sigmoid的导数可以表示为： $$S^{‘}(x)&#x3D;S(x)(1-S(x))$$ 所以，bo的梯度为： $$\\frac{\\partial L}{\\partial b_o}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))$$ 这里先提一下矩阵乘法的梯度计算公式： $$Y&#x3D;X\\times W$$ $$\\frac{\\partial L}{\\partial X}&#x3D;\\frac{\\partial L}{\\partial Y}\\times W^T$$ $$\\frac{\\partial L}{\\partial W}&#x3D;X^T\\times\\frac{\\partial L}{\\partial Y}$$ 由此可以进一步计算出$x_t、W_{ox}、h_{t-1}、W_{oh}$的梯度分别为： $$\\frac{\\partial L}{\\partial h_{t-1}}&#x3D;\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))\\right]\\times W_{oh}^T$$ $$\\frac{\\partial L}{\\partial x_t}&#x3D;\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))\\right]\\times W_{ox}^T$$ $$\\frac{\\partial L}{\\partial W_{oh}}&#x3D;h_{t-1}^T\\times\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot\\left(o_t\\cdot(1-o_t)\\right)\\right]$$ $$\\frac{\\partial L}{\\partial W_{ox}}&#x3D;x_t^T\\times\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot\\left(o_t\\cdot(1-o_t)\\right)\\right]$$ 3.1.2 其它门的反向传播由公式（1）可知，tanh(Ct)的梯度为ht的梯度与输出门ot的点积，根据tanh函数的梯度（$(\\tanh x)’&#x3D;1-\\tanh^2x$）可以得到Ct的梯度为： $$\\frac{\\partial L}{\\partial C_t}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot o_t\\cdot(1-tanh^2(C_t)) \\tag{2}$$ 由于Ct计算公式为： $$C_t&#x3D;f_t\\cdot C_{t-1}+i_t\\cdot \\stackrel{\\sim }{C_t}$$ 所以可以得到$f_t、C_{t-1}、i_t、\\stackrel{\\sim }{C_t}$的梯度： $$\\frac{\\partial L}{\\partial f_t}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot C_{t-1}$$ $$\\frac{\\partial L}{\\partial C_{t-1}}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot f_t \\tag{3}$$ $$\\frac{\\partial L}{\\partial i_t}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot \\stackrel{\\sim }{C_t}$$ $$\\frac{\\partial L}{\\partial \\stackrel{\\sim }{C_t}}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot i_t$$ 现在得到了输入门、遗忘门、候补细胞状态的梯度后，可以按照和输出门一样的方法，先计算激活函数内部的梯度，再根据矩阵乘法的梯度公式进一步计算出$W、h_{t-1}、x_t$的梯度，这里不再赘述。 由于$h_{t-1}和x_t$在前向传播中被使用了4次，因此这里也会收到来自各个门和候补细胞状态的4个梯度，将其相加即可得到总梯度。 3.2 其它LSTM单元的反向传播其它LSTM单元来说，其与最后一个LSTM单元的区别在于ht和Ct的梯度来源不同。 前面提到，最后一个LSTM单元的ht只来自输出，而在3.1.2的最后，我们计算出了最后一个LSTM单元对$h_{t-1}$的梯度，因此倒数第二个LSTM单元则需要加上该梯度。此外，其输出方向也可能有梯度传来，比如多层LSTM的情况： 此时，下层LSTM每一个单元的输出，是上层每一个LSTM单元的输入。因此上层LSTM单元计算完$x_t$的梯度后，将会传回给下层LSTM单元。 同样的，Ct的梯度也来自两个方向，一方面来自当前LSTM单元中ht传来的梯度，如公式(2)；另一方面来自下一个LSTM单元，如公式(3)。 3.3 权重的更新前面说过，每一个LSTM单元都使用相同的权重，按照链式法则，这些权重也要接收来自每一个单元传来的梯度。 前面已经介绍了LSTM单元如何计算各个门和细胞状态的权重($W_h、W_x$以及$b$)的梯度，但是此时并不能更新这些权重，因为在前面的单元中，计算$h_{t-1}$和$x_t$的梯度时，需要用到这些权重，如果在后面的LSTM单元中修改了权重，那么前面的单元就无法得知原先的权重是多少。正确的做法是将每个单元计算出的权重的梯度累计起来，最后再更新权重参数。 4 LSTM反向传播的C++实现为了简单易懂，这里只用最基础的数组方式去实现。 4.1 矩阵乘法等基本函数实现矩阵乘法实现，经典的三个for循环，使用template指定输入输出数组的大小从而实现各种大小的矩阵相乘（后面全部函数都采用这种template的方法）： 矩阵乘法实现1234567891011template&lt;unsigned char row1,unsigned char col1,unsigned char col2&gt;void matrix_multiply(float inputarr1[row1][col1],float inputarr2[col1][col2],float outputarr[row1][col2]) &#123;\tfor(int i=0;i&lt;row1;i++)&#123; for(int j=0;j&lt;col2;j++)&#123; float temp=0.0; for(int k=0;k&lt;col1;k++) temp += inputarr1[i][k] * inputarr2[k][j]; outputarr[i][j] = temp; &#125;\t&#125;&#125; $A^T\\times B$的实现，可以理解为第一个矩阵的某一列与第二个矩阵的每一列相乘，作为结果的某一行的每一列数据，结果的行个数与输入的列个数一致： 图片演示 代码实现123456789101112template&lt;unsigned char row1,unsigned char col1,unsigned char col2&gt;void matrix_transpose1_multiply(float inputarr1[row1][col1],float inputarr2[row1][col2],float outputarr[col1][col2])&#123;\tfor(int i=0;i&lt;col1;i++)&#123; for(int j=0;j&lt;col2;j++)&#123; float temp = 0.0; for(int k=0;k&lt;row1;k++) temp += inputarr1[k][i] * inputarr2[k][j]; outputarr[i][j] = temp; &#125;\t&#125;&#125; $A\\times B^T$的实现，可以理解为第一个矩阵的某一行与第二个矩阵的每一行相乘，作为结果的某一行的每一列数据，结果的行个数与输入的行个数一致： 图片演示 代码实现123456789101112template&lt;unsigned char row1,unsigned char col1,unsigned char row2&gt;void matrix_transpose2_multiply(float inputarr1[row1][col1],float inputarr2[row2][col1],float outputarr[row1][row2])&#123;\tfor(int i=0;i&lt;row1;i++)&#123; for(int j=0;j&lt;row2;j++)&#123; float temp = 0.0; for(int k=0;k&lt;col1;k++) temp += inputarr1[i][k] * inputarr2[j][k]; outputarr[i][j] = temp; &#125;\t&#125;&#125; 4.2 反向传播的实现代码4.2.1 计算门和候补细胞状态的梯度由于计算各个门和候补细胞状态梯度的过程是类似的，因此将其封装为一个函数： 计算门和候补细胞状态的梯度1234567891011121314151617181920212223242526272829303132333435363738template&lt;int input_dims,int output_dims&gt;void compute_doors_gredient(float delta[1][output_dims],float K[1][output_dims],float gate[1][output_dims],\tfloat h_weight[output_dims][output_dims],float x_weight[input_dims][output_dims],\tfloat h_input[1][output_dims],float x_input[1][input_dims],\tfloat ht_delta[1][output_dims],float input_delta[1][input_dims],\tfloat delta_wx[input_dims][output_dims],float delta_wh[output_dims][output_dims],float delta_wb[output_dims],\tint choice=0)&#123;\tfloat delta_gate[1][output_dims];\tdot_product&lt;1,output_dims&gt;(delta,K,delta_gate); float delta_inner[1][output_dims];\tif(choice==0) sigmoid_delta&lt;1,output_dims&gt;(delta_gate,gate,delta_inner);\telse tanh_delta&lt;1,output_dims&gt;(delta_gate,gate,delta_inner);\tfloat temp_delta_wx[input_dims][output_dims];\tfloat temp_delta_wh[output_dims][output_dims]; matrix_transpose2_multiply&lt;1,output_dims,output_dims&gt;(delta_inner,h_weight,ht_delta);\tmatrix_transpose2_multiply&lt;1,output_dims,input_dims&gt;(delta_inner,x_weight,input_delta);\tmatrix_transpose1_multiply&lt;1,input_dims,output_dims&gt;(x_input,delta_inner,temp_delta_wx);\tmatrix_transpose1_multiply&lt;1,output_dims,output_dims&gt;(h_input,delta_inner,temp_delta_wh); for(int i=0;i&lt;output_dims;i++)&#123; delta_wb[i] += delta_inner[0][i];\t&#125;\tfor(int i=0;i&lt;input_dims;i++) for(int j=0;j&lt;output_dims;j++) delta_wx[i][j] += temp_delta_wx[i][j]; for(int i=0;i&lt;output_dims;i++) for(int j=0;j&lt;output_dims;j++) delta_wh[i][j] += temp_delta_wh[i][j];&#125; 在计算输出门时，delta为$\\frac{\\partial L}{\\partial h_t}$，其余均为$\\frac{\\partial L}{\\partial C_t}$。 K对于输出门来说是$tanh(C_t)$，对于输入门来说是$\\stackrel{\\sim }{C_t}$，依此类推，忘记的可以回头看前面公式，都有一个点积的操作，而后才是求激活函数的梯度，最后求各个权重和输入的梯度。部分函数并未给出，写起来也很简单，比如点积就是矩阵对应位置相乘。 这里权重的梯度使用“+&#x3D;”也是前面提到的，权重的梯度需要先累加起来。 4.2.2 LSTM反向传播的主函数LSTM反向传播主函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123template&lt;int timesteps,int input_dims,int output_dims&gt;void lstm_backpropagation(float learning_rate,float info[timesteps][6*output_dims],float delta[timesteps][output_dims],\tfloat input[timesteps][input_dims],float cells_output[timesteps][output_dims],float first_ht[1][output_dims],\tfloat wx_i[input_dims][output_dims],float wh_i[output_dims][output_dims],float b_i[output_dims],\tfloat wx_f[input_dims][output_dims],float wh_f[output_dims][output_dims],float b_f[output_dims],\tfloat wx_c[input_dims][output_dims],float wh_c[output_dims][output_dims],float b_c[output_dims],\tfloat wx_o[input_dims][output_dims],float wh_o[output_dims][output_dims],float b_o[output_dims],\tfloat delta_input[timesteps][input_dims])&#123;\tfloat delta_ct[1][output_dims];\tfloat delta_ht[1][output_dims]; //save total loss\tfloat delta_wx_i[input_dims][output_dims];float delta_wh_i[output_dims][output_dims];float delta_b_i[output_dims];\tfloat delta_wx_f[input_dims][output_dims];float delta_wh_f[output_dims][output_dims];float delta_b_f[output_dims];\tfloat delta_wx_c[input_dims][output_dims];float delta_wh_c[output_dims][output_dims];float delta_b_c[output_dims];\tfloat delta_wx_o[input_dims][output_dims];float delta_wh_o[output_dims][output_dims];float delta_b_o[output_dims]; //initial\tfor(int i=0;i&lt;input_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; delta_wx_i[i][j] = 0;\tdelta_wx_f[i][j] = 0;\tdelta_wx_c[i][j] = 0;\tdelta_wx_o[i][j] = 0; &#125;\t&#125; for(int i=0;i&lt;output_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; delta_wh_i[i][j] = 0;\tdelta_wh_f[i][j] = 0;\tdelta_wh_c[i][j] = 0;\tdelta_wh_o[i][j] = 0; &#125; delta_b_i[i] = 0;\tdelta_b_f[i] = 0;\tdelta_b_c[i] = 0;\tdelta_b_o[i] = 0; delta_ht[0][i] = 0;\tdelta_ct[0][i] = 0;\t&#125; for(int i=timesteps-1;i&gt;=0;i--)\t&#123; float old_ct[1][output_dims]; float input_gate[1][output_dims]; float forget_gate[1][output_dims]; float prepared_cell[1][output_dims]; float output_gate[1][output_dims]; float tanc_t[1][output_dims]; float input_x[1][input_dims]; float input_h[1][output_dims]; //recover information for(int j=0;j&lt;output_dims;j++)&#123; old_ct[0][j] = info[i][j]; input_gate[0][j] = info[i][output_dims+j]; forget_gate[0][j] = info[i][2*output_dims+j]; prepared_cell[0][j] = info[i][3*output_dims+j]; output_gate[0][j] = info[i][4*output_dims+j]; tanc_t[0][j] = info[i][5*output_dims+j]; if(i!=0) input_h[0][j] = cells_output[i-1][j]; // old ht else input_h[0][j] = first_ht[0][j]; &#125; for(int j=0;j&lt;input_dims;j++) input_x[0][j] = input[i][j]; //the final cell&#x27;s del_ht only from delta //other&#x27;s delta_ht from latter del_ht and delta for(int j=0;j&lt;output_dims;j++)&#123; delta_ht[0][j] += delta[i][j]; &#125; //the final cell&#x27;s del_ct only from ht //other&#x27;s delta_ct from latter_cell&#x27;s del_ct and current del_ht float temp_delta_ct[1][output_dims]; dot_product&lt;1,output_dims&gt;(delta_ht,output_gate,temp_delta_ct); tanh_delta&lt;1,output_dims&gt;(temp_delta_ct,tanc_t,temp_delta_ct); for(int j=0;j&lt;output_dims;j++)&#123; delta_ct[0][j] += temp_delta_ct[0][j]; &#125; float delta_ht_i[1][output_dims]; float delta_ht_f[1][output_dims]; float delta_ht_c[1][output_dims]; float delta_ht_o[1][output_dims]; float delta_input_i[1][input_dims]; float delta_input_f[1][input_dims]; float delta_input_c[1][input_dims]; float delta_input_o[1][input_dims]; compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,prepared_cell,input_gate, wh_i,wx_i,input_h,input_x,delta_ht_i,delta_input_i,delta_wx_i,delta_wh_i,delta_b_i); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,old_ct, forget_gate, wh_f,wx_f,input_h,input_x,delta_ht_f,delta_input_f,delta_wx_f,delta_wh_f,delta_b_f); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,input_gate, prepared_cell,wh_c,wx_c,input_h,input_x,delta_ht_c,delta_input_c,delta_wx_c,delta_wh_c,delta_b_c,1); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ht,tanc_t, output_gate, wh_o,wx_o,input_h,input_x,delta_ht_o,delta_input_o,delta_wx_o,delta_wh_o,delta_b_o); //compute delta_ht for previous cell for(int j=0;j&lt;output_dims;j++) delta_ht[0][j] = delta_ht_i[0][j]+delta_ht_f[0][j]+delta_ht_c[0][j]+delta_ht_o[0][j]; //compute delta_input(if use multi-layer lstm) for(int j=0;j&lt;input_dims;j++) delta_input[i][j] = delta_input_i[0][j]+delta_input_f[0][j]+delta_input_c[0][j]+delta_input_o[0][j]; //compute delta_ct for previous cell dot_product&lt;1,output_dims&gt;(delta_ct,forget_gate,delta_ct);\t&#125; //update weight\tfor(int i=0;i&lt;input_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; wx_i[i][j] -= learning_rate*delta_wx_i[i][j]; wx_f[i][j] -= learning_rate*delta_wx_f[i][j]; wx_c[i][j] -= learning_rate*delta_wx_c[i][j]; wx_o[i][j] -= learning_rate*delta_wx_o[i][j]; &#125;\t&#125; for(int i=0;i&lt;output_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; wh_i[i][j] -= learning_rate*delta_wh_i[i][j]; wh_f[i][j] -= learning_rate*delta_wh_f[i][j]; wh_c[i][j] -= learning_rate*delta_wh_c[i][j]; wh_o[i][j] -= learning_rate*delta_wh_o[i][j]; &#125; b_i[i] -= learning_rate*delta_b_i[i]; b_f[i] -= learning_rate*delta_b_f[i]; b_c[i] -= learning_rate*delta_b_c[i]; b_o[i] -= learning_rate*delta_b_o[i];\t&#125;&#125; “for(int i&#x3D;timesteps-1;i&gt;&#x3D;0;i- -)”即是从最后一个LSTM单元开始计算梯度： 首先需要还原前向传播计算的信息，比如前面说的计算输出门ot的梯度时，需要知道$tanh(C_t)$的值，计算ot门内部梯度时，也需要知道ot的值，其余同理。对于$h_{t-1}$，我在前向传播时将每一个LSTM单元的输出保存在cells_output中，因此代码中“i-1”代表上一次LSTM单元的输出。而对于第一个LSTM单元，如果ht和Ct采用零初始化，那么这里$h_{t-1}$就为0，但是Stateful LSTM等情况时，会出现不为0的情况，因此这里直接增加了一个参数，将最开始的ht传进来。 参数delta是每一个LSTM单元的输出方向传来的梯度，如果采用多层LSTM，那么就是下一层LSTM的delta_input。如果是LSTM接全连接这种，就只有最后一个LSTM单元的输出有梯度，其余为0。 接下来和公式推导过程一样，先计算ht的梯度，再计算Ct的梯度，最后计算权重和输入的梯度。delta_ht和delta_ct是后一个LSTM单元传给前一个单元的ht和Ct的梯度。本来需要判断是否是最后一个LSTM单元，如果是最后一个，那么delta_ht就等于输出方向传来的梯度，delta_ct就等于ht方向计算的梯度。其余的LSTM单元则是”+&#x3D;”，因为梯度来自两方面。但由于delta_ht和delta_ct本身就是零初始化的，所以全部采用”+&#x3D;”也没问题，还省去了判断过程。 在for循环结束后，也就是所有LSTM单元计算完权重的梯度后，再更新权重。 5 写在最后注意权重初始化问题，当初我在实现的过程中，不太关注初始化，导致梯度消失很快。 第一次写博客，有问题欢迎在评论区提出","tags":["LSTM反向传播"],"categories":["深度学习"]},{"path":"/css/mouse.css","content":"/*鼠标样式*/ body, html { cursor: url(/cur/pointer.cur), auto !important; } /* a, */ img { cursor: url(/cur/link.cur), auto !important; } summary { cursor: url(/cur/link.cur), /* !important可能导致bug */ auto !important; } p { cursor: url(/cur/text.cur), auto; } details { cursor: url(/cur/text.cur), auto; } /*a标签*/ a:hover { cursor: url(/cur/link.cur), auto; } svg { cursor: url(/cur/link.cur), auto; } a.top:hover { cursor: url(/cur/link.cur), auto; } a.buttom:hover { cursor: url(/cur/link.cur), auto; } input:hover{ cursor: url(/cur/text.cur), auto; } /*按钮*/ button:hover { cursor: url(/cur/link.cur), auto; } /*i标签*/ i:hover { cursor: url(/cur/link.cur), auto; } /* copy按钮 */ div.copy-btn { cursor: url(/cur/link.cur), auto !important; } /*页脚a标签*/ #footer-wrap a:hover { cursor: url(/cur/link.cur), auto; } /*分页器*/ #pagination .page-number:hover { cursor: url(/cur/link.cur), auto; } /*头部的导航栏*/ #nav .site-page:hover { cursor: url(/cur/link.cur), auto; } /*鼠标样式END*/"}]