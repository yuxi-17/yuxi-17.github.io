[{"path":"/posts/1/","content":"test"},{"title":"NEAL(Network-Aware Locality Scheduling)论文简要解读","path":"/posts/1128674413/","content":"Network-Aware Locality Scheduling是一种借助元启发式算法的共流（Coflow）调度方法，原文链接： 论文链接https://ieeexplore.ieee.org/abstract/document/9329172 1.共流调度的基本背景在大数据处理过程中，通常存在聚合操作，比如MapReduce的Reduce操作，Spark的groupByKey等等。这些操作需要将相同键的值合并在一起，比如单词统计任务，可以将一堆文件分给不同节点单独处理，此时就会出现“单词-出现次数”的键值对。在reduce阶段将不同节点处理的结果合并在一起，属于同一个键的值加起来，就可以得到这堆文件中每个单词的出现次数。 当然，键值对不一定是key和单个值，也可能存在key-tuple的情况，设想有这么一个任务，需要统计不同人的消费记录，不同节点可以用来处理不同平台的消费记录，然后生成“ 用户-消费记录元组”的键值对，比如节点1统计外卖平台，生成键值对{用户1 : (消费记录1,消费记录2…)}、{用户2 : (消费记录1,消费记录2…)}等等。节点2可以用来统计网购平台数据、节点3统计游戏平台数据，诸如此类。那么在reduce的时候，需要将每个用户在不同平台的消费记录合并在一起，比如属于用户1的所有平台消费记录需要进行合并，那么这时就需要思考一个问题，这些数据如今分布在各个节点，究竟交给哪个节点来完成这个合并操作，这个节点为汇聚节点。 下面是三种分配策略，在我们这个例子中，节点1、2、3就是不同平台的消费数据，而$a^{b}$代表用户a（Key），b为消费记录的个数（或者说元组大小、数据量），比如$1^{3}$代表用户1，有三条消费记录： 在a中，根据简单的hash函数来决定汇聚节点。而b和c是其它的调度策略，区别在于用户2的汇聚节点不同（b选择汇聚到节点1，c选择汇聚到节点2）。这里的调度策略并不是重点，关键是如何去评价哪个调度策略是好的： 假如按照最小传输数据量来看，a、b、c传输的元组大小分别为8、7、6，那么SP2是最优的。 从另一个角度来看，SP2中节点1需要发送4个数据给节点2（Key为1的有3个数据，Key为2的有1个数据），节点2和节点3分别需要发送1个数据。这些在节点之间传输的数据流组成了共流（coflow），需要等待共流中的所有数据流都传输完毕后才能开始下一步的shuffle操作。这时节点1发送给节点2的数据太多，成为了瓶颈，相比之下，SP1中每个节点需要发送的数据分别为3、3、1。假如发送一个数据需要的通信时间为1，则SP2的通信时间为4，SP1的为3，从这个角度看SP1又好一点。 但是假如节点2的带宽减小为原来的一半，那么此时SP1的通信时间为6，而SP2不变（因为SP2的节点2只需要发送1个数据，带宽减半后需要2个时间，仍然比节点1的4个时间要短）。 这里可以进一步做一个带宽的调整，如下图所示： 对于SP2，由于节点1需要4个时间，其余两个节点只需要1个时间，那么可以将这两个节点的带宽缩小，使得它也需要4个时间才能完成传输。这样的好处在于给其它任务腾出了一些带宽，此外数据在同一个时刻到达也更加方便处理。 从上面几个情况可以得到，在选择汇聚节点时，需要考虑传输的数据量，以及可用的带宽。 查看节点之间的可用带宽，以及为节点之间的传输分配带宽，可以通过SDN的Openflow控制器来实现。 2.共流调度的一般情况现在假设有n个节点，p个Key，如下图左边表格所示，格子里面的数字代表数据量，也就是前面说的元组大小，考虑到不是所有节点都会有某一个Key的数据，因此存在数据量为0的情况。 对于每一个Key来说，其都需要选择一个汇聚节点，如右上方表格所示，每一列只有一个1，其余为0，代表每一个Key有且只有一个汇聚节点，其它节点如果拥有该Key的数据，就需要发送给汇聚节点。这可以进一步简化为右下角的表格，对于Key1来说，其选择节点n为汇聚节点，而Key2选择节点1，Key_p选择节点2。 如此，这个问题就变成了一个NP难题，共有$n^{p}$种情况（一共p个Key，每个Key都有n种情况）。当然，这里每一列真的需要考虑n种情况吗？ 如果对于Key_k来说，只有节点j拥有该Key的数据，那就可以直接确定汇聚节点为j，而不需要考虑放在其它位置。 但是只要有两个（或两个以上）节点拥有该Key的数据，就是n种情况了，因为它可以选择汇聚在这两个节点中的一个，也可以选择汇聚在其它节点，如同图1a的Key2一样，放在了节点3，即使节点3没有该Key的数据。为什么会出现这种情况呢？因为存在带宽的限制。对于星型拓扑来说（节点m&lt;-&gt;Openflow交换机&lt;-&gt;节点n），节点的带宽可分为输入带宽和输出带宽。假如节点1和节点2的输入带宽比较小（或者有其它数据也需要用到这部分带宽），而它们的输出带宽充足，同时节点3的输入带宽也充足，那么就可以选择将节点1和节点2的数据发送给节点3，这样也许比节点1发送给节点2或者节点2发送给节点1要快。 实际情况中node的数量可能很多，出现两个或者两个以上的节点拥有同一个Key的数据是很常见的，因此这确实是一个NP难题。 当每一个Key都确定了汇聚节点后，就可以开始计算它的传输时间了。 节点i的发送时间：$$\\sum_{j&#x3D;1,j eq i}^n\\sum_{k&#x3D;1}^p\\frac{h_{ik}}{r_i}x_{jk}\\quad\\forall i\\tag{2.1}$$ $\\sum_{k&#x3D;1}^p$指的是每一个Key的数据都需要考虑，$\\sum_{j&#x3D;1,j eq i}^n $指的是传输到除了节点i的所有节点，$h_{ik}$指的是节点i中Key_k的数据，$x_{jk}$指的Key_k的数据是否传输给节点j。$\\sum_{j&#x3D;1,j eq i}^n\\sum_{k&#x3D;1}^ph_{ik}x_{jk}$指的是需要发送给其它节点的数据量，而$r_i$指的是节点i的发送带宽（输出带宽）。 同理，节点j的接收时间为：$$\\sum_{i&#x3D;1,i eq j}^n\\sum_{k&#x3D;1}^p\\frac{h_{ik}}{r_j}x_{jk}\\quad\\forall j\\tag{2.2}$$ 也就是每一个Key的数据都需要考虑是否以j为汇聚节点，如果是，那么其它所有拥有该Key数据的节点都需要发数据发给节点j，$r_j$指的是节点j的接收带宽（输入带宽）。 当然，两个节点之间的传输时间并不是发送时间+接收时间，因为发送和接收可以同时进行。 现在考虑最慢流传输时间$\\tau$，所有节点都在$\\tau$时间内完成发送，也在$\\tau$时间内完成接收，即：$$\\tau &#x3D; max(\\sum_{j&#x3D;1,j eq i}^n\\sum_{k&#x3D;1}^p\\frac{h_{ik}}{r_i}x_{jk}\\quad\\forall i\\quad,\\quad\\sum_{i&#x3D;1,i eq j}^n\\sum_{k&#x3D;1}^p\\frac{h_{ik}}{r_j}x_{jk}\\quad\\forall j)\\tag{2.3}$$通过带宽调整，使得所有流的传输时间都等于这个最大时间。 3.元启发式算法求解调度问题现在的问题在于求解每一个Key的汇聚节点的位置，使得最慢流传输时间$\\tau$最小。 这样的问题可以使用元启发式算法求解，其实细心的人早已发现，这和八皇后问题很像，都是每一列只能放一个，只不过八皇后的评估值是位于同一行或同一斜的皇后的对数，而这里是公式2.3，目的都是使得评估值最小。 元启发式算法很多，比如遗传算法、爬山法、模拟退火算法等等，论文则采用了鲸鱼优化算法(WOA)。 关于鲸鱼优化算法的解释网上有很多，因此这里不再赘述，将问题分析到这一步其实已经很明确了。 仅代表博主个人理解，有错误欢迎指正！","tags":["Network-Aware Locality Scheduling,NEAL,Coflow调度"],"categories":["论文解读"]},{"title":"STARNUMA论文解读","path":"/posts/618860/","content":"对STARNUMA论文（StarNUMA: Mitigating NUMA Challenges with Memory Pooling）的各部分概括和解释，原文链接： https://faculty.cc.gatech.edu/~adaglis/files/papers/starnuma_micro24.pdfhttps://faculty.cc.gatech.edu/~adaglis/files/papers/starnuma_micro24.pdf 1.论文背景大型多Socket（插槽）系统中，每一个Socket包括具有多个内核的CPU，以及本地内存DRAM，不同Socket之间通过相干链路互连（比如Intel的UPI，超路径互连）。下面是16-Socket系统的示意图： 分为4个机箱，每个机箱有4个Socket，每个Socket连接到机箱间互连ASIC（FLEX ASIC），机箱间的一致性链路称为 NUMALinks，每个机箱可以单跳访问另一个机箱。 在多socket系统中，每个处理器都能直接访问任何内存位置，但内存访问的延迟和带宽特性极大地取决于目标内存位置与访问处理器之间的距离。本地内存访问需要80ns，机箱内需要130ns，机箱间需要360ns。由此造成了非均匀内存访问（NUMA）效应，因此需要通过智能数据放置和迁移以最大限度减少远程内存访问，也就是将处理器需要的页面放到离处理器比较近的内存中。 但是部分页面共享程度高，因此其没有“home” （主）socket位置，也就是无论其放到哪个处理器附近，都无法避免远程内存访问，这种页面叫游离页。此外，由于共享程度高，其可能会在不同处理器之间不断迁移，产生了一定的代价。 如上图所示，78% 的页面有四个或更少的共享者，这表明智能迁移策略可以将页面访问控制在机箱内。然而，虽然只有 7% 的页面有 8 个以上的共享者，但 68% 的内存访问都集中在这些共享页面上，而所有 16 个Socket共享的 2% 页面占所有访问的 36%。这说明有相当一部分访问针对的是少数广泛共享的页面，而访问这些广泛共享的页面大概率需要机箱间通信。 简单地复制这些页面到不同的内存中需要很高的软件复杂性和开销来保持内存一致性，因为它们大部分是读写页。 2.STARNUMA设计2.1 内存池STARNUMA 通过 CXL 附加内存池扩展了多Socket系统，每一个Socket通过CXL链接连接到共享内存池，组成了一个星型结构。 由于 CXL 3.x 的底层物理层是 PCIe 6.0，因此一条 8 通道的 CXL 链路可为每个方向提供 64GB&#x2F;s 的原始带宽，而实际带宽约为 40GB&#x2F;s。 图3显示了访问共享内存池所需的时间，其中每个CXL端口需要25ns，重定时器（远程访问所需）20ns，链路上时间10ns，片上网络、内部仲裁逻辑和一致性目录的时间估计为 20ns，总共100ns。如果再加上在处理器上消耗的时间和DRAM访问的时间，则总时间为180ns，仍然比机箱间互连所需的360ns时间要短。 如果Socket数目继续增加，则需要CXL交换机，总时间可能来到了275ns。 STARNUMA一方面缩短了内存访问的时间（如果数据在池上，其访问延迟比机箱间访问延时小），另一方面由于提供了新的CXL链路使得系统总带宽增加了。 2.2 一致性 图4展示了因为目录一致性而产生的缓存块传输： （1）假如socket（R）需要访问内存的某块数据，但该块数据所在的页的主节点是socket（H），因此需要先访问socket（H）。这个是由TLB决定的，该块数据的虚拟地址映射的物理地址在socket（H）的内存范围中。 但在此之前，可能另一个Socket（O）已经访问并占有了该块（通过向该块写入数据），因此需要让Socket（O）将最新的数据传输给Socket（R）。路径为红色R-&gt;H-&gt;O-&gt;R。 （2）假如该页的主节点在共享内存池上，则同样需要先访问socket（O），但并不是由socket（O）直接发送给socket（R），而是需要传输回内存池中，所以路径为蓝色R-&gt;H-&gt;O-&gt;H-&gt;R。 由于socket与内存池之间的链路是CXL，提供了更高的传输速率，所以即使看上去第（2）中情况路径更长，但实际传输延迟要小。（2）传输延迟为200ns，（1）为333ns。 2.3 内存访问监控和页面迁移 STARNUMA需要一种机制去检测游离页，也就是广泛共享的页面。 如图5所示，将内存分成了多个区域，每个区域包括多个页面。 每个区域都在内存中拥有一个元数据区域（metadata region），其包括两部分内容，一部分用于统计哪些socket访问了该区域的数据，另一部分是统计该区域总访问次数的计数器（一共i位）。当i为0时，即不关注访问次数，只关注该区域的共享程度。 TLB中也有i位的计数器附件，在完成 LLC（最后一级缓存）缺失（意味着需要访问内存）加载后，相应 TLB 附件条目的计数器会递增。 当 TLB 条目被删除时，PTW会将附件条目的值添加到内存元数据区域的相应计数器中。为了获取从未在 TLB 中被删除的热页面计数器，所有 TLB 附件条目都有一个标记位（即图中的periodic marker bit），每个迁移阶段（1s）定期设置一次。当访问已设置标记的 TLB 条目时，PTW 会将附件条目的值添加到相应的元数据区域位置，并重置标记。 图6是页面迁移的算法，这里Region_Trackers应该是按照i位计数器进行从高到低的排序。 当一个区域的访问次数超过阈值，就需要将该区域进行迁移。如果该区域共享者太多，就放到内存池中。如果目标的内存满了，就移出一个访问次数较低的区域（for循环遍历找到第一个访问次数低于阈值的即可，所以并不一定是访问次数最低的）。 另外，一个区域如果经常迁移，那么其处于ping-ponging状态，如果继续迁移下去，可能并不会提高多少效率，反而带来迁移开销。 总的迁移次数是有限的，并不是所有需要迁移的页面都会被处理。 迁移会导致物理地址发生变化，从而导致页表更新和TLB shotdowns，即使一个内核的TLB中没有需要删除的条目，但它也会收到来自处理迁移任务的内核的中断，内核接收中断进入内核模式需要几千个周期的时间。因此需要一种硬件支持，引入共享 TLB 目录，只向在其 TLB 中实际缓存了迁移页转换的必要内核发送 TLB shotdowns。同时设计还应包括最小内核扩展，允许完全在硬件中处理 TLB 条目失效，无需操作系统干预。具体的硬件处理方法并没描述，其它论文可能有该问题的解决方案。 区域的大小需要权衡，如果区域较小，那就需要很多区域，相应的元数据区域也会变多，内存占用增加，同时使用算法进行迁移时遍历元数据区域所需的时间也会增加，但小区域有着更高的跟踪精度，使得其包含的页面绝大多数都是被广泛共享的。 3.评估方法3.1 基于采样的仿真 分为3个步骤： （1）步骤A在真实机器上部署目标工作负载，并跟踪每个应用线程的执行情况，包括指令和内存访问跟踪。内存跟踪以10亿条指令为间隔记录，并将其称为一个阶段，此外还记录了每个阶段的前 1 亿条指令。 （2）步骤B利用步骤A的内存跟踪，在每一个阶段结束后根据页面访问的情况做出数据迁移决策（也就是2.3中的算法），将页面进行迁移，从而生成新的“页面-内存”映射。 （3）步骤C进行并行时序仿真，也就是每个阶段单独仿真。因为步骤B在每个阶段结束后迁移了页面，所以每个阶段都有自己的“页面-内存”映射（也就是页面在内存中的位置不一样）。在这种映射下，轻内核根据步骤A收集的内存访问情况发出内存访问请求，详细内核则运行步骤A收集的指令来发出内存请求，关于轻内核和详细内核见下一节。 这里的Baseline也会有页面迁移，只不过没有CXL共享内存池。 3.2 混合模式仿真 即使采用基于采样的方法，在周期级模拟整个多socket系统仍然非常困难，因此需要缩小socket规模。 只有一个socket的内核有完整的微架构细节（详细内核），它用来执行步骤A记录的1亿条指令，执行指令的时候会产生内存访问。其它socket里面的内核是轻内核，它们只根据步骤A的内存跟踪向系统注入内存请求就行了，注入速率根据模拟socket的IPC进行调节。 每个轻型socket都有一个 LLC 大小的高速缓存（高一级缓存是低一级缓存的子集，所以有LLC就已经足够了），用于支持一致性建模和过滤对内存的访问，还有一个详细的内存控制器模型，用于处理针对其内存范围的内存请求，同时准确捕捉内存调度和争用对性能的影响。在 LLC 和每个处理器的内存控制器之间有一个互连仿真模块，对插座间拓扑结构及其相关链路延迟和带宽限制进行建模，从而捕捉争用&#x2F;排队效应。在收到新的内存访问请求时，通过查询步骤 B 生成的页面映射输入，确定目标插槽。然后，互连模块根据请求的源插槽和目标插槽，确定请求必须穿越的链路顺序。穿越互连后，请求要么进入目标插槽的内存控制器队列，要么根据需要触发一致性事件。 3.3 迁移开销建模将 HI 门限设定为 20K，LO设置为1K，并在每个迁移阶段根据迁移次数限制对其进行动态调整。在页面迁移过程中，对该页面的所有内存访问都会停止，直到迁移完成。周期级时序仿真（步骤C）仅涵盖每个十亿条指令阶段的前 1 亿条指令。因此，在时序仿真中只对每个阶段前 10%的迁移进行建模，相当于运行部分指令来测试迁移后的效果。 3.4 系统配置 表1和表2对比了完整的16-socket系统和模拟系统之间的差异，这里将内存池允许的数据量限制为每个工作负载所用内存的 20%。 3.5 工作负载 图形分析：广度优先搜索 (BFS)、连接组件 (CC)、单源最短路径 (SSSP) 和三角形计数 (TC)。 高性能计算：分钟空间中的全文索引（FMI）和部分排序对齐（POA）。 数据服务： 使用Masstree 高性能 KeyValue 存储器，数据集为 100GB，密钥流行度分布均匀，读写比率为 50&#x2F;50。 事务处理：在Silo 内存数据库管理系统上部署的带有 64 个仓库的 TPCC。 表里总结了工作负载的IPC和LLC MPKI（1000条指令中LLC缺失个数）。括号中为仅使用本地内存时的IPC，单插槽和 16 插槽执行之间 2-10 倍的 IPC 差距说明了 NUMA 效应对性能的影响。 4.测试结果4.1 主要结果 图 8b 显示了测得的 AMAT（内存平均访问时间），分解为unloaded延迟以及插槽间和 CXL 链路竞争造成的延迟，unloaded延迟是没有任何竞争情况下的预期 AMAT，可以看到，排队和竞争会对性能造成一定的影响。 图8a显示了STARNUMA降低了AMAT，提高了IPC。STARNUMA 通过两种不同的方式减少延迟：(i) 直接减少unloaded延迟（平均减少跳数）；(ii) CXL 链路提供了额外带宽，减少争用，进而减少排队延迟。其中TC 和 FMI 属于计算密集型，LLC MPKI 较低，争用较少，主要利用(i)进行加速。而SSSP 和 BFS 受带宽限制，主要利用(ii)。其余工作负载利用了(i)和(ii)。POA所有访问都是本地访问，因此不会发生迁移，也不会将数据放入池中，只需要使用首次接触页面放置策略即可放置NUMA效应。另外T0虽然无法识别哪些区域访问次数最多，但其可以判断共享程度，也获得了获得了 T16 的大部分收益。 图 8c 显示了内存访问按类型划分的情况。STARNUMA 中的大多数 BT（一致性块传输） 都是通过池路径完成的，这比 3 跳套接字间传输平均快 30%，不过因更快的 BT 而减少的 AMAT 分数微乎其微。 表4强调了池作为一种新的数据放置选项的重要性，除 POA 外，大多数页面都被迁移到池中（这里应该是访问的页面位于池中占总访问页面的比例）。 4.2 与 Oracular 静态页面放置的比较 静态页面放置方法是根据每个工作负载访问模式的先验知识进行放置，运行时不再进行页面迁移。比如根据3.1步骤A的内存跟踪得到，某一页面在socket0中访问的最多，因此把它放在socket0的内存上。 图9对比了使用静态StarNUMA，和普通StarNUMA（首次接触放置+动态迁移），以及静态Baseline的性能。 静态初始页面放置和动态迁移差不多，甚至会优于动态迁移，因为其消除了迁移开销，而且说明页面的共享模式不会随着时间的推移而发生剧烈变化。 因为假如一个页面在阶段0-阶段1时被socket0大量访问，而后在阶段2-阶段10时被socket1访问，只不过从总访问量来说，socket0访问该页面次数大于socket1，因此静态方法将该页面放到socket0的内存上，导致阶段2-阶段10 socket1只能进行远程内存访问，相比之下，假如采取迁移策略，阶段2的结尾处会将该页面迁移到socket1的内存上，阶段3-阶段10 socket1就可以进行本地内存访问。从这个角度出发，静态页面放置策略应该会比动态迁移的效果要差，但图9与此相反，说明上面假设的情况很少出现，也就是共享模式不会发生剧烈变化，在这种情况下，迁移带来的效果不多，反而会造成了一定的开销导致IPC下降。 而静态Baseline不如动态迁移StarNUMA，这是因为游离页无论放在哪里都无法避免远程内存访问，放在共享内存池中才能提高效率。 4.3 内存池延迟的影响 图10假设了加上CXL交换机的延迟（90ns）对性能造成的影响，除了TC外（前面说过，TC主要靠减少延迟来提高速率），其余影响不是很大。 4.4 带宽可用性的影响 对比了3种配置： （1）提高Baseline中UPI 和 NUMALink 的带宽，使得其总带宽与STARNUMA中的总带宽一致。 （2）直接将Baseline中UPI 和 NUMALink 的带宽增加一倍，增加的带宽远高于STARNUMA 16 个 CXL 链路带来的额外带宽。 （3）将 CXL 链路带宽减半至 20GB&#x2F;s（即CXL通道个数减半）。 除了BFS工作负载外，其余的Baseline ISO-BW（配置1）和Baseline 2X BW（配置2）均没有超过StarNUMA的性能。BFS的Baseline 2X BW超过StarNUMA是因为前面提到，其工作主要受带宽约束，此外其均匀利用了机箱内和机箱间的链路，而StarNUMA因为将热门页面放到池上，使得内存访问的很大一部分集中在池的 CXL 链路上，而机箱内和机箱间的链路利用率低。 尽管池带宽减少了一半，但 STARNUMA Half-BW 性能也能比Baseline（虚线）高（除了BFS）。 因此，单纯提高机箱内和机箱间的链路带宽意义不大，解决池上的竞争问题更加重要（4.1的图8b）。 4.5 内存池容量的影响前面内存池容量设置为工作负载内存占用的20%，现在把内存池的容量设置为与socket内存大小一致，此时内存池容量为每个工作负载内存占用容量的1&#x2F;17。 除了FMI外，大多数工作负载对池的大小并不敏感，这表明大部分远程访问针对的是一小部分页面。 4.6 页面复制与内存池 页面复制带来了在软件中保持页面一致性的主要开销。只有在不担心内存容量浪费的情况下，只读共享页才是良好的复制候选页。 在BFS任务中，大多数访问都以共享读写页面为目标（图2），导致池目录平均每 50 个周期处理一次一致性操作。在这种情况下，复制和软件一致性会产生过高的开销。 图13中TC则与之相反，大多数内存访问都是对共享只读页面的访问，虽然不存在一致性问题，但复制会造成过大的内存容量压力。 其余工作负载的页面访问行为介于 BFS 和 TC 之间。 复制对于只读的流浪页面是有效的，因为这些页面所占的访问比例较高，占用的内存空间较小。页面复制和 STARNUMA 可以作为互补技术联合使用。 4.7 评估方法的影响 前面使用的是默认配置 [SC1]：每十亿条指令为一个阶段，每个内核在仿真的每个阶段只模拟其中1亿条指令。 而现在增加两组配置： [SC2]：模拟3倍更详细的指令（每个阶段运行3亿条指令）。 [SC3]：系统规模翻倍（每个插槽8个内核，2倍内存&#x2F;互连带宽，以及针对 内核数8 × socket数16 &#x3D; 128 个线程的指令跟踪）。 图14表示即使是规模更大、成本更高的仿真配置（SC2 和 SC3）也证实了 STARNUMA 的潜力，产生了类似或更好的结果。 5. 写在最后由于大多数内存访问的目标页面放在池上（4.1节表4），会导致排队和竞争，从而对性能造成影响（4.1节图b），同时由于访问池都需要用到CXL链路，出现机箱内和机箱间链路带宽闲置的现象（4.4节）。 虽然可以通过对只读的共享页面进行复制，缓解共享内存池的压力（4.6节），但是带宽闲置问题并没有给出合适的解决方案。其实可以通过适当减少内存池中的页面数量来平衡带宽，减轻池的压力，如4.5节中缩小池的容量。或者也可以考虑调整2.3节算法的共享数阈值来限制池中的页面数量，2.3的算法中当页面的共享者个数大于8（socket数的一半）时，即可放入池中，可以测试不同阈值情况下的性能。当然，也不是非得充分利用机箱内和机箱间的链路，还需要考虑池的排队和竞争延迟有没有超过CXL链路减少的延迟。","tags":["STARNUMA"],"categories":["论文解读"]},{"title":"PYNQ教程二:使用AXI总线实现PS与PL交互","path":"/posts/855715585/","content":"ZYNQ架构分为处理器系统（PS）和可编程逻辑（PL）两大部分，PS运行在ARM Cortex A9处理器上，PL相当于FPGA，本文介绍如何使用AXI总线实现PS与PL之间的交互。 1 PL部分设计PL设计可以使用Vivado HLS开发，或者Verilog开发，这里介绍使用Vivado HLS的方式。 设计一个简单的模块，代码如下： 123456789void test(float input[5],float num,float output[5])&#123;#pragma HLS INTERFACE s_axilite port=return#pragma HLS INTERFACE s_axilite port=num#pragma HLS INTERFACE m_axi depth=5 port=input offset=slave bundle=INPUT#pragma HLS INTERFACE m_axi depth=5 port=output offset=slave bundle=OUTPUT\tfor(int i=0;i&lt;5;i++) output[i] = input[i] * num;&#125; 函数功能很简单，就是将输入数组乘上某一个数后交给输出数组。 pragma解释： “port&#x3D;return”表示配置块级协议，”s_axilite”表示将ap_start等信号放置于AXI从接口寄存器中。 其余几个都是配置端口协议，”m_axi”即配置为AXI主接口，”s_axilite”即配置为AXI从接口。因为主接口访问数据时需要知道数据所在的地址，”offset&#x3D;slave”表示将地址放在AXI从接口寄存器中。”bundle”可以将接口合并，这里bundle各不相同，也就意味着生成两个AXI主接口。 上面就是HLS生成的IP核。在HLS中打开solution1&#x2F;impl&#x2F;ip&#x2F;drivers&#x2F;*_v1_0&#x2F;src，找到一个以hw.h结尾的文件，打开后可以看到AXI从接口寄存器信息（也可以使用SDK的方式，但是我觉得这样更方便）。 2 Vivado Block Design 可以看到，ZYNQ有两个AXI_GP主接口、两个AXI_GP从接口、四个AXI_HP从接口和一个AXI_ACP从接口，这些接口的区别可以参考下面的视频： AXI接口讲解https://www.bilibili.com/video/BV1FD4y1E79q/?spm_id_from=333.999.0.0 在这里，由于HLS生成的IP核有一个AXI从接口和两个AXI主接口，因此这里ZYNQ至少使用一个AXI主接口和一个AXI从接口，然后使用AXI Interconnect或AXI SmartConnect实现多主多从之间的连接。这里可以使用Vivado的自动连接功能。 上图是一种连接方式，也可以使用AXI_GP从接口，不一定要用AXI_HP从接口。 3 PYNQ PS端代码PS主接口和PL从接口： 根据pynq.io官方的描述：”In an overlay, peripherals connected to the AXI General Purpose ports will have their registers or address space mapped into the system memory map. With PYNQ, the register, or address space of an IP can be accessed from Python using the MMIO class.” 原网址如下： MMIOhttps://pynq.readthedocs.io/en/latest/pynq_libraries/mmio.html 翻译过来就是：对于连接至PYNQ AXI_GP主接口的IP，PS可以在Python代码中使用MMIO类访问它们。 PS从接口和PS主接口： 同样根据官方的说法：”IP connected to the AXI Master (HP or ACP ports) has access to PS DRAM. Before IP in the PL accesses DRAM, some memory must first be allocated (reserved) for the IP to use and the size, and address of the memory passed to the IP. An array in Python, or Numpy, will be allocated somewhere in virtual memory. The physical memory address of the allocated memory must be provided to IP in the PL. “（这里第一句话感觉应该不是AXI Master而是AXI Slave才对） 原网址如下： Allocatehttps://pynq.readthedocs.io/en/latest/pynq_libraries/allocate.html 翻译过来就是：连接至PYNQ从接口的IP可以访问DDR内存（DRAM），不过PS需要预先分配出一段内存，并将这段内存的物理地址传递给PL的IP核。 接下来开始代码实现过程： （1）烧写底层设计的bit流文件： 1234from pynq import Overlayol = Overlay(&#x27;ps_pl.bit&#x27;)ol? ”ol?“可以查看overlay的信息： 可以看到，识别到hls生成的IP核”test_0“，该IP核被封装为一个MMIO类对象，可以通过”ol.test_0?”查看其详细信息，mmio类内部有”write”和”read”函数实现读写功能。 （2）使用allocate函数分配一段内存，该内存可以当作一个numpy数组来使用，同时device_address属性为其物理地址： 1234567from pynq import allocateimport numpy as npinput_buffer = allocate(shape=(5,),dtype=np.float32)input_buffer.flush()output_buffer = allocate(shape=(5,),dtype=np.float32)output_buffer.flush() （3）将内存地址传递给PL的IP核： 123test_ip = ol.test_0test_ip.write(0x10,input_buffer.device_address)test_ip.write(0x20,output_buffer.device_address) 前面hw.h文件显示的AXI从接口寄存器地址中，0x10和0x20分别是输入和输出的地址，因此这里将PS分配的内存的物理地址分别写入到这两个地方中。 （4）写入输入数据： 123456inputs = np.array([1,2,3,4,5],dtype=np.float32)np.copyto(input_buffer,inputs)import structnum = struct.unpack(&#x27;&lt;I&#x27;,struct.pack(&#x27;&lt;f&#x27;,3))[0]test_ip.write(0x18,num) 这里num不能直接写入3，因为HLS定义了num是float类型。但是也不能写入3.0，否则会报下面的错误： ValueError:&nbsp;Data&nbsp;type&nbsp;must&nbsp;be&nbsp;int&nbsp;or&nbsp;bytes. 因为MMIO类的”write”函数不能写入float类型数据。所以这里使用struct做了一个变换，从float类型转换成int类型，struct的用法可参考： struct函数解析https://zhuanlan.zhihu.com/p/387421751 （5）启动IP核工作，等待其工作结束： 12345import timetest_ip.write(0x0,1)while(test_ip.read(0x0)&amp;0x2 == 0): time.sleep(0.001) 0x0位置是控制信号，写入1则ap_start置1，IP核开始工作。而第2位为ap_done，因此这里采用轮询的方式，等待第2位为1则IP核工作结束。 （6）获取输出结果： 1234outputs = np.zeros([5])np.copyto(outputs,output_buffer)print(outputs) 结果如下：","tags":["PYNQ"],"categories":["PYNQ教程"]},{"title":"PYNQ教程一:GPIO的使用","path":"/posts/1649035944/","content":"GPIO通常用于读写寄存器，与外部设备的交互等场景。本文从PS GPIO和AXI GPIO两方面分别介绍它们的配置方法，以及如何使用PYNQ编写Python代码实现数据交互。 1 PS GPIO1.1 PS GPIO的配置PS GPIO分为MIO和EMIO，其中MIO共54bit，EMIO共64bit。 其使用方法可以参考以下的网站： pynq官方文档https://pynq.readthedocs.io/en/latest/pynq_package/pynq.gpio.html pynq.io网站上文档非常全面介绍了Pynq的各种函数，后面的AXI_GPIO和Arduino_IO都可以在这里找到相关的函数解析 MIO和EMIO的使用https://blog.csdn.net/qq_35712169/article/details/106038000 总的来说，MIO无需配置，其位置已经固定，可以将其配置为GPIO口，但是需要注意的是其与其它接口复用（比如USB、ETH等），可能存在冲突导致无法使用的情况，而且位置固定后反而不方便使用，所以我个人更推荐使用EMIO的方式。 EMIO没有固定的引脚，其配置方法较简单，打开ZYNQ7 IP核配置页面，然后按下面的路径打开： 打开后拉到最下面就可以看到GPIO的配置，这里可以选择使用的EMIO的位数： 配置完后可以看到IP核多了一个GPIO_0接口，然后可以将这些端口绑定到特定的引脚上。 为了方便测试读写功能，可以将数据传输给其它的IP核，并获取其输出。这里可以使用“Utility Vector Logic”构建一个与门，输入输出大小为2。 GPIO_O只有一个，而与门有两个输入，因此这里需要通过两个“slice”IP核将64位数据分割开来，slice0配置如下，取输入的0-1位输出。 slice1配置类似，取输入2-3位输出。而后将这两个slice的输出连接到与门。 与门的输出并不能直接与GPIO_I连接。前面提到，EMIO只有64位，这里GPIO_O和GPIO_I显然是共用的，如果将与门的输出连接到GPIO_I，那么其结果将会发给低位，也就是0-1位，但是前面0-1位已经被GPIO_O用于输出了，当然如果0-1位配置为“inout”，那就没问题。但是后续的Python代码中PS GPIO的某一位只能被配置为“in”或“out”，而不是“inout”。因此这里需要加一个偏移项，将与门的输出传给GPIO_I的4-5位，从而避免该冲突，这可以通过“Constant”和“Concat”IP核实现偏移功能。 如图所示，“Constant”设置了4位，值为0，通过“Concat“将两者拼接起来，组成6位数据，与门的输出在最高两位。 在Synthesis中，会出现以下警告，但是无关紧要，如果不想出现这样的警告，可以将EMIO的位数设置为6位。 1.2 PS GPIO Python代码 首先烧写bit流文件，然后导入GPIO库，配置0-3位为输出，4-5位为输入，通过”read“和”write“函数实现GPIO的读写功能。 前面的内容参考了下面的视频，所以不太理解的也可以查看下面的视频讲解 youtube视频讲解https://youtu.be/a5NnLozPEI0?si=TiMc5d3H6u_wqcK1 1.3 Arduino的GPIO 使用Arduino的IO时，同样不需要配置，Python代码也比较简单，可以参考以下网站： Arduino IOhttps://rkblog.dev/posts/python/using-gpio-usb-and-hdmi-pynq-z2-board/ 2 AXI GPIO2.1 AXI GPIO配置AXI GPIO的配置相比于EMIO来说要简单一点，且可以同时使用多个AXI GPIO，灵活性很强。 IP核配置可以参考以下文章： AXI GPIO IP核配置https://blog.csdn.net/u011565038/article/details/138836478 我通常启用双通道，一个All Outputs，一个All Inputs，这样就不需要管Default Tri State Value这一栏了。 连接也很简单，先手动将AXI GPIO的输出与输入和非门连接，接下来使用Vivado的自动连接就行了。 2.2 AXI GPIO Python代码 首先烧写bit流文件，获取overlay信息： 得到AXI GPIO IP核的名字后，即可使用AxiGPIO函数将该IP核实例化为一个类对象，而后根据Pynq提供的函数实现读写功能。 也可以参考下面的官方例程： AXI GPIO官方案例https://github.com/Xilinx/PYNQ_Workshop/blob/master/Session_4/2_axi_gpio.ipynb 写在最后可以看到，文中引用了很多其它网站，毕竟在最初的学习中，也是广泛搜索了各种网站，然后才慢慢明白。所以我精选出这些我觉得有用的网站，可以省去广泛查找的过程，同时也更加全面。当然，有一定基础的看一下我写的也大概明白了。虽然不是很难，但是硬件的东西，别人说一下可能几分钟就知道了，如果自己去找资料看文档，可能要花费几个小时甚至于几天的时间。","tags":["PYNQ"],"categories":["PYNQ教程"]},{"title":"LSTM反向传播推导及C++代码实现","path":"/posts/1552645455/","content":"之前在找资料时候发现大多数文章都只讲了LSTM前向传播，比较少有关于反向传播的内容，而且反向传播有很多公式，需要一步步地推才容易理解，另外在实现过程中还有一些小细节需要注意。 1 LSTM基础概念时间步（time_steps)：可以认为是输入的次数，有多少次输入就有“多少个”LSTM单元，但从本质上来看却是一个LSTM单元的复用。 如同简单RNN一样： 只有一个单元，但为了分析起来简单，所以通常将它展开，展开后看似有多个单元，但其实这些单元内部的权重完全一样，只不过中间状态和输入不一样。 输入维度(input_size)：每个单元接收的输入shape是1×input_size。 隐藏层大小&#x2F;输出维度（hidden_size&#x2F;output_size)：隐藏层大小决定了ht和Ct的大小，由于输出和ht相等，所以也决定了输出的大小。 例：假如存在一个任务，需要通过前面10个时刻的值来预测第11个时刻的值，此时time_steps为10，input_size为1（每次输入1个值，输入10次）。 2 LSTM权重大小计算前面提到，本质上只有一个LSTM单元，因此权重大小和time_steps无关，只与input_size和hidden_size有关。 LSTM单元需要计算三个门+一个候补细胞状态，每次都是分别将输入(1×input_size)和ht(1×hidden_size)与对应的权重进行矩阵相乘，输出大小都为1×hidden_size，最后加上偏置，因此可以推得权重大小为： $$ P&#x3D;4\\times(inputsize\\times hiddensize+hiddensize\\times hiddensize+hiddensize) $$ 3 LSTM反向传播推导3.1 最后一个LSTM单元LSTM最后一个单元其ht的梯度只来自输出。 3.1.1 输出门ot的反向传播由于输出门计算公式为： $$h_t&#x3D;o_t\\cdot\\tanh(C_t) \\tag{1}$$ 因此求输出门ot的梯度时，只需要将ht的梯度与tanh(Ct)相乘（点乘），这里tanh(Ct)的结果在前向传播时计算得到，因此需要在前向传播时将其保存下来： $$\\frac{\\partial L}{\\partial o_t}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)$$ 接着求输出门ot内部的梯度，由于ot门计算公式为： $$o_t&#x3D;\\sigma(x_t\\times W_{ox}+h_{t-1}\\times W_{oh}+b_o)$$ 所以求ot门内部的梯度，即可得到bo的梯度，由于sigmoid的导数可以表示为： $$S^{‘}(x)&#x3D;S(x)(1-S(x))$$ 所以，bo的梯度为： $$\\frac{\\partial L}{\\partial b_o}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))$$ 这里先提一下矩阵乘法的梯度计算公式： $$Y&#x3D;X\\times W$$ $$\\frac{\\partial L}{\\partial X}&#x3D;\\frac{\\partial L}{\\partial Y}\\times W^T$$ $$\\frac{\\partial L}{\\partial W}&#x3D;X^T\\times\\frac{\\partial L}{\\partial Y}$$ 由此可以进一步计算出$x_t、W_{ox}、h_{t-1}、W_{oh}$的梯度分别为： $$\\frac{\\partial L}{\\partial h_{t-1}}&#x3D;\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))\\right]\\times W_{oh}^T$$ $$\\frac{\\partial L}{\\partial x_t}&#x3D;\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))\\right]\\times W_{ox}^T$$ $$\\frac{\\partial L}{\\partial W_{oh}}&#x3D;h_{t-1}^T\\times\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot\\left(o_t\\cdot(1-o_t)\\right)\\right]$$ $$\\frac{\\partial L}{\\partial W_{ox}}&#x3D;x_t^T\\times\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot\\left(o_t\\cdot(1-o_t)\\right)\\right]$$ 3.1.2 其它门的反向传播由公式（1）可知，tanh(Ct)的梯度为ht的梯度与输出门ot的点积，根据tanh函数的梯度（$(\\tanh x)’&#x3D;1-\\tanh^2x$）可以得到Ct的梯度为： $$\\frac{\\partial L}{\\partial C_t}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot o_t\\cdot(1-tanh^2(C_t)) \\tag{2}$$ 由于Ct计算公式为： $$C_t&#x3D;f_t\\cdot C_{t-1}+i_t\\cdot \\stackrel{\\sim }{C_t}$$ 所以可以得到$f_t、C_{t-1}、i_t、\\stackrel{\\sim }{C_t}$的梯度： $$\\frac{\\partial L}{\\partial f_t}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot C_{t-1}$$ $$\\frac{\\partial L}{\\partial C_{t-1}}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot f_t \\tag{3}$$ $$\\frac{\\partial L}{\\partial i_t}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot \\stackrel{\\sim }{C_t}$$ $$\\frac{\\partial L}{\\partial \\stackrel{\\sim }{C_t}}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot i_t$$ 现在得到了输入门、遗忘门、候补细胞状态的梯度后，可以按照和输出门一样的方法，先计算激活函数内部的梯度，再根据矩阵乘法的梯度公式进一步计算出$W、h_{t-1}、x_t$的梯度，这里不再赘述。 由于$h_{t-1}和x_t$在前向传播中被使用了4次，因此这里也会收到来自各个门和候补细胞状态的4个梯度，将其相加即可得到总梯度。 3.2 其它LSTM单元的反向传播其它LSTM单元来说，其与最后一个LSTM单元的区别在于ht和Ct的梯度来源不同。 前面提到，最后一个LSTM单元的ht只来自输出，而在3.1.2的最后，我们计算出了最后一个LSTM单元对$h_{t-1}$的梯度，因此倒数第二个LSTM单元则需要加上该梯度。此外，其输出方向也可能有梯度传来，比如多层LSTM的情况： 此时，下层LSTM每一个单元的输出，是上层每一个LSTM单元的输入。因此上层LSTM单元计算完$x_t$的梯度后，将会传回给下层LSTM单元。 同样的，Ct的梯度也来自两个方向，一方面来自当前LSTM单元中ht传来的梯度，如公式(2)；另一方面来自下一个LSTM单元，如公式(3)。 3.3 权重的更新前面说过，每一个LSTM单元都使用相同的权重，按照链式法则，这些权重也要接收来自每一个单元传来的梯度。 前面已经介绍了LSTM单元如何计算各个门和细胞状态的权重($W_h、W_x$以及$b$)的梯度，但是此时并不能更新这些权重，因为在前面的单元中，计算$h_{t-1}$和$x_t$的梯度时，需要用到这些权重，如果在后面的LSTM单元中修改了权重，那么前面的单元就无法得知原先的权重是多少。正确的做法是将每个单元计算出的权重的梯度累计起来，最后再更新权重参数。 4 LSTM反向传播的C++实现为了简单易懂，这里只用最基础的数组方式去实现。 4.1 矩阵乘法等基本函数实现矩阵乘法实现，经典的三个for循环，使用template指定输入输出数组的大小从而实现各种大小的矩阵相乘（后面全部函数都采用这种template的方法）： 矩阵乘法实现1234567891011template&lt;unsigned char row1,unsigned char col1,unsigned char col2&gt;void matrix_multiply(float inputarr1[row1][col1],float inputarr2[col1][col2],float outputarr[row1][col2]) &#123;\tfor(int i=0;i&lt;row1;i++)&#123; for(int j=0;j&lt;col2;j++)&#123; float temp=0.0; for(int k=0;k&lt;col1;k++) temp += inputarr1[i][k] * inputarr2[k][j]; outputarr[i][j] = temp; &#125;\t&#125;&#125; $A^T\\times B$的实现，可以理解为第一个矩阵的某一列与第二个矩阵的每一列相乘，作为结果的某一行的每一列数据，结果的行个数与输入的列个数一致： 图片演示 代码实现123456789101112template&lt;unsigned char row1,unsigned char col1,unsigned char col2&gt;void matrix_transpose1_multiply(float inputarr1[row1][col1],float inputarr2[row1][col2],float outputarr[col1][col2])&#123;\tfor(int i=0;i&lt;col1;i++)&#123; for(int j=0;j&lt;col2;j++)&#123; float temp = 0.0; for(int k=0;k&lt;row1;k++) temp += inputarr1[k][i] * inputarr2[k][j]; outputarr[i][j] = temp; &#125;\t&#125;&#125; $A\\times B^T$的实现，可以理解为第一个矩阵的某一行与第二个矩阵的每一行相乘，作为结果的某一行的每一列数据，结果的行个数与输入的行个数一致： 图片演示 代码实现123456789101112template&lt;unsigned char row1,unsigned char col1,unsigned char row2&gt;void matrix_transpose2_multiply(float inputarr1[row1][col1],float inputarr2[row2][col1],float outputarr[row1][row2])&#123;\tfor(int i=0;i&lt;row1;i++)&#123; for(int j=0;j&lt;row2;j++)&#123; float temp = 0.0; for(int k=0;k&lt;col1;k++) temp += inputarr1[i][k] * inputarr2[j][k]; outputarr[i][j] = temp; &#125;\t&#125;&#125; 4.2 反向传播的实现代码4.2.1 计算门和候补细胞状态的梯度由于计算各个门和候补细胞状态梯度的过程是类似的，因此将其封装为一个函数： 计算门和候补细胞状态的梯度1234567891011121314151617181920212223242526272829303132333435363738template&lt;int input_dims,int output_dims&gt;void compute_doors_gredient(float delta[1][output_dims],float K[1][output_dims],float gate[1][output_dims],\tfloat h_weight[output_dims][output_dims],float x_weight[input_dims][output_dims],\tfloat h_input[1][output_dims],float x_input[1][input_dims],\tfloat ht_delta[1][output_dims],float input_delta[1][input_dims],\tfloat delta_wx[input_dims][output_dims],float delta_wh[output_dims][output_dims],float delta_wb[output_dims],\tint choice=0)&#123;\tfloat delta_gate[1][output_dims];\tdot_product&lt;1,output_dims&gt;(delta,K,delta_gate); float delta_inner[1][output_dims];\tif(choice==0) sigmoid_delta&lt;1,output_dims&gt;(delta_gate,gate,delta_inner);\telse tanh_delta&lt;1,output_dims&gt;(delta_gate,gate,delta_inner);\tfloat temp_delta_wx[input_dims][output_dims];\tfloat temp_delta_wh[output_dims][output_dims]; matrix_transpose2_multiply&lt;1,output_dims,output_dims&gt;(delta_inner,h_weight,ht_delta);\tmatrix_transpose2_multiply&lt;1,output_dims,input_dims&gt;(delta_inner,x_weight,input_delta);\tmatrix_transpose1_multiply&lt;1,input_dims,output_dims&gt;(x_input,delta_inner,temp_delta_wx);\tmatrix_transpose1_multiply&lt;1,output_dims,output_dims&gt;(h_input,delta_inner,temp_delta_wh); for(int i=0;i&lt;output_dims;i++)&#123; delta_wb[i] += delta_inner[0][i];\t&#125;\tfor(int i=0;i&lt;input_dims;i++) for(int j=0;j&lt;output_dims;j++) delta_wx[i][j] += temp_delta_wx[i][j]; for(int i=0;i&lt;output_dims;i++) for(int j=0;j&lt;output_dims;j++) delta_wh[i][j] += temp_delta_wh[i][j];&#125; 在计算输出门时，delta为$\\frac{\\partial L}{\\partial h_t}$，其余均为$\\frac{\\partial L}{\\partial C_t}$。 K对于输出门来说是$tanh(C_t)$，对于输入门来说是$\\stackrel{\\sim }{C_t}$，依此类推，忘记的可以回头看前面公式，都有一个点积的操作，而后才是求激活函数的梯度，最后求各个权重和输入的梯度。部分函数并未给出，写起来也很简单，比如点积就是矩阵对应位置相乘。 这里权重的梯度使用“+&#x3D;”也是前面提到的，权重的梯度需要先累加起来。 4.2.2 LSTM反向传播的主函数LSTM反向传播主函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123template&lt;int timesteps,int input_dims,int output_dims&gt;void lstm_backpropagation(float learning_rate,float info[timesteps][6*output_dims],float delta[timesteps][output_dims],\tfloat input[timesteps][input_dims],float cells_output[timesteps][output_dims],float first_ht[1][output_dims],\tfloat wx_i[input_dims][output_dims],float wh_i[output_dims][output_dims],float b_i[output_dims],\tfloat wx_f[input_dims][output_dims],float wh_f[output_dims][output_dims],float b_f[output_dims],\tfloat wx_c[input_dims][output_dims],float wh_c[output_dims][output_dims],float b_c[output_dims],\tfloat wx_o[input_dims][output_dims],float wh_o[output_dims][output_dims],float b_o[output_dims],\tfloat delta_input[timesteps][input_dims])&#123;\tfloat delta_ct[1][output_dims];\tfloat delta_ht[1][output_dims]; //save total loss\tfloat delta_wx_i[input_dims][output_dims];float delta_wh_i[output_dims][output_dims];float delta_b_i[output_dims];\tfloat delta_wx_f[input_dims][output_dims];float delta_wh_f[output_dims][output_dims];float delta_b_f[output_dims];\tfloat delta_wx_c[input_dims][output_dims];float delta_wh_c[output_dims][output_dims];float delta_b_c[output_dims];\tfloat delta_wx_o[input_dims][output_dims];float delta_wh_o[output_dims][output_dims];float delta_b_o[output_dims]; //initial\tfor(int i=0;i&lt;input_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; delta_wx_i[i][j] = 0;\tdelta_wx_f[i][j] = 0;\tdelta_wx_c[i][j] = 0;\tdelta_wx_o[i][j] = 0; &#125;\t&#125; for(int i=0;i&lt;output_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; delta_wh_i[i][j] = 0;\tdelta_wh_f[i][j] = 0;\tdelta_wh_c[i][j] = 0;\tdelta_wh_o[i][j] = 0; &#125; delta_b_i[i] = 0;\tdelta_b_f[i] = 0;\tdelta_b_c[i] = 0;\tdelta_b_o[i] = 0; delta_ht[0][i] = 0;\tdelta_ct[0][i] = 0;\t&#125; for(int i=timesteps-1;i&gt;=0;i--)\t&#123; float old_ct[1][output_dims]; float input_gate[1][output_dims]; float forget_gate[1][output_dims]; float prepared_cell[1][output_dims]; float output_gate[1][output_dims]; float tanc_t[1][output_dims]; float input_x[1][input_dims]; float input_h[1][output_dims]; //recover information for(int j=0;j&lt;output_dims;j++)&#123; old_ct[0][j] = info[i][j]; input_gate[0][j] = info[i][output_dims+j]; forget_gate[0][j] = info[i][2*output_dims+j]; prepared_cell[0][j] = info[i][3*output_dims+j]; output_gate[0][j] = info[i][4*output_dims+j]; tanc_t[0][j] = info[i][5*output_dims+j]; if(i!=0) input_h[0][j] = cells_output[i-1][j]; // old ht else input_h[0][j] = first_ht[0][j]; &#125; for(int j=0;j&lt;input_dims;j++) input_x[0][j] = input[i][j]; //the final cell&#x27;s del_ht only from delta //other&#x27;s delta_ht from latter del_ht and delta for(int j=0;j&lt;output_dims;j++)&#123; delta_ht[0][j] += delta[i][j]; &#125; //the final cell&#x27;s del_ct only from ht //other&#x27;s delta_ct from latter_cell&#x27;s del_ct and current del_ht float temp_delta_ct[1][output_dims]; dot_product&lt;1,output_dims&gt;(delta_ht,output_gate,temp_delta_ct); tanh_delta&lt;1,output_dims&gt;(temp_delta_ct,tanc_t,temp_delta_ct); for(int j=0;j&lt;output_dims;j++)&#123; delta_ct[0][j] += temp_delta_ct[0][j]; &#125; float delta_ht_i[1][output_dims]; float delta_ht_f[1][output_dims]; float delta_ht_c[1][output_dims]; float delta_ht_o[1][output_dims]; float delta_input_i[1][input_dims]; float delta_input_f[1][input_dims]; float delta_input_c[1][input_dims]; float delta_input_o[1][input_dims]; compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,prepared_cell,input_gate, wh_i,wx_i,input_h,input_x,delta_ht_i,delta_input_i,delta_wx_i,delta_wh_i,delta_b_i); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,old_ct, forget_gate, wh_f,wx_f,input_h,input_x,delta_ht_f,delta_input_f,delta_wx_f,delta_wh_f,delta_b_f); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,input_gate, prepared_cell,wh_c,wx_c,input_h,input_x,delta_ht_c,delta_input_c,delta_wx_c,delta_wh_c,delta_b_c,1); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ht,tanc_t, output_gate, wh_o,wx_o,input_h,input_x,delta_ht_o,delta_input_o,delta_wx_o,delta_wh_o,delta_b_o); //compute delta_ht for previous cell for(int j=0;j&lt;output_dims;j++) delta_ht[0][j] = delta_ht_i[0][j]+delta_ht_f[0][j]+delta_ht_c[0][j]+delta_ht_o[0][j]; //compute delta_input(if use multi-layer lstm) for(int j=0;j&lt;input_dims;j++) delta_input[i][j] = delta_input_i[0][j]+delta_input_f[0][j]+delta_input_c[0][j]+delta_input_o[0][j]; //compute delta_ct for previous cell dot_product&lt;1,output_dims&gt;(delta_ct,forget_gate,delta_ct);\t&#125; //update weight\tfor(int i=0;i&lt;input_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; wx_i[i][j] -= learning_rate*delta_wx_i[i][j]; wx_f[i][j] -= learning_rate*delta_wx_f[i][j]; wx_c[i][j] -= learning_rate*delta_wx_c[i][j]; wx_o[i][j] -= learning_rate*delta_wx_o[i][j]; &#125;\t&#125; for(int i=0;i&lt;output_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; wh_i[i][j] -= learning_rate*delta_wh_i[i][j]; wh_f[i][j] -= learning_rate*delta_wh_f[i][j]; wh_c[i][j] -= learning_rate*delta_wh_c[i][j]; wh_o[i][j] -= learning_rate*delta_wh_o[i][j]; &#125; b_i[i] -= learning_rate*delta_b_i[i]; b_f[i] -= learning_rate*delta_b_f[i]; b_c[i] -= learning_rate*delta_b_c[i]; b_o[i] -= learning_rate*delta_b_o[i];\t&#125;&#125; “for(int i&#x3D;timesteps-1;i&gt;&#x3D;0;i- -)”即是从最后一个LSTM单元开始计算梯度： 首先需要还原前向传播计算的信息，比如前面说的计算输出门ot的梯度时，需要知道$tanh(C_t)$的值，计算ot门内部梯度时，也需要知道ot的值，其余同理。对于$h_{t-1}$，我在前向传播时将每一个LSTM单元的输出保存在cells_output中，因此代码中“i-1”代表上一次LSTM单元的输出。而对于第一个LSTM单元，如果ht和Ct采用零初始化，那么这里$h_{t-1}$就为0，但是Stateful LSTM等情况时，会出现不为0的情况，因此这里直接增加了一个参数，将最开始的ht传进来。 参数delta是每一个LSTM单元的输出方向传来的梯度，如果采用多层LSTM，那么就是下一层LSTM的delta_input。如果是LSTM接全连接这种，就只有最后一个LSTM单元的输出有梯度，其余为0。 接下来和公式推导过程一样，先计算ht的梯度，再计算Ct的梯度，最后计算权重和输入的梯度。delta_ht和delta_ct是后一个LSTM单元传给前一个单元的ht和Ct的梯度。本来需要判断是否是最后一个LSTM单元，如果是最后一个，那么delta_ht就等于输出方向传来的梯度，delta_ct就等于ht方向计算的梯度。其余的LSTM单元则是”+&#x3D;”，因为梯度来自两方面。但由于delta_ht和delta_ct本身就是零初始化的，所以全部采用”+&#x3D;”也没问题，还省去了判断过程。 在for循环结束后，也就是所有LSTM单元计算完权重的梯度后，再更新权重。 5 写在最后注意权重初始化问题，当初我在实现的过程中，不太关注初始化，导致梯度消失很快。 第一次写博客，有问题欢迎在评论区提出","tags":["LSTM反向传播"],"categories":["深度学习"]},{"path":"/css/mouse.css","content":"/*鼠标样式*/ body, html { cursor: url(/cur/pointer.cur), auto !important; } /* a, */ img { cursor: url(/cur/link.cur), auto !important; } summary { cursor: url(/cur/link.cur), /* !important可能导致bug */ auto !important; } p { cursor: url(/cur/text.cur), auto; } h2:hover { cursor: url(/cur/text.cur), auto; } h1.text.title { cursor: url(/cur/text.cur), auto; } details { cursor: url(/cur/text.cur), auto; } /*a标签*/ a:hover { cursor: url(/cur/link.cur), auto; } svg { cursor: url(/cur/link.cur), auto; } a.top:hover { cursor: url(/cur/link.cur), auto; } a.buttom:hover { cursor: url(/cur/link.cur), auto; } a.link-card.plain:hover { cursor: url(/cur/link.cur), auto; } input:hover{ cursor: url(/cur/text.cur), auto; } /*按钮*/ button:hover { cursor: url(/cur/link.cur), auto; } /*i标签*/ i:hover { cursor: url(/cur/link.cur), auto; } /* copy按钮 */ div.copy-btn { cursor: url(/cur/link.cur), auto !important; } /*页脚a标签*/ #footer-wrap a:hover { cursor: url(/cur/link.cur), auto; } /*分页器*/ #pagination .page-number:hover { cursor: url(/cur/link.cur), auto; } /*头部的导航栏*/ #nav .site-page:hover { cursor: url(/cur/link.cur), auto; } /*鼠标样式END*/"},{"path":"/baidu_verify_codeva-jbLr6Kbtit.html","content":"3b1030ef3cbd0208ff08b59fa18d9249"}]