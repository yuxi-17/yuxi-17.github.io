[{"title":"LSTM反向传播推导及C++代码实现","path":"/2024/06/02/LSTM反向传播推导及代码实现/","content":"之前在找资料时候发现大多数文章都只讲了LSTM前向传播，比较少有关于反向传播的内容，而且反向传播有很多公式，需要一步步地推才容易理解，另外在实现过程中还有一些小细节需要注意。 1 LSTM基础概念时间步（time_steps)：可以认为是输入的次数，有多少次输入就有“多少个”LSTM单元，但从本质上来看却是一个LSTM单元的复用。 如同简单RNN一样： 只有一个单元，但为了分析起来简单，所以通常将它展开，展开后看似有多个单元，但其实这些单元内部的权重完全一样，只不过中间状态和输入不一样。 输入维度(input_size)：每个单元接收的输入shape是1×input_size。 隐藏层大小&#x2F;输出维度（hidden_size&#x2F;output_size)：隐藏层大小决定了ht和Ct的大小，由于输出和ht相等，所以也决定了输出的大小。 例：假如存在一个任务，需要通过前面10个时刻的值来预测第11个时刻的值，此时time_steps为10，input_size为1（每次输入1个值，输入10次）。 2 LSTM权重大小计算前面提到，本质上只有一个LSTM单元，因此权重大小和time_steps无关，只与input_size和hidden_size有关。 LSTM单元需要计算三个门+一个候补细胞状态，每次都是分别将输入(1×input_size)和ht(1×hidden_size)与对应的权重进行矩阵相乘，输出大小都为1×hidden_size，最后加上偏置，因此可以推得权重大小为： $$ P&#x3D;4\\times(inputsize\\times hiddensize+hiddensize\\times hiddensize+hiddensize) $$ 3 LSTM反向传播推导3.1 最后一个LSTM单元LSTM最后一个单元其ht的梯度只来自输出。 3.1.1 输出门ot的反向传播由于输出门计算公式为： $$h_t&#x3D;o_t\\cdot\\tanh(C_t) \\tag{1}$$ 因此求输出门ot的梯度时，只需要将ht的梯度与tanh(Ct)相乘（点乘），这里tanh(Ct)的结果在前向传播时计算得到，因此需要在前向传播时将其保存下来： $$\\frac{\\partial L}{\\partial o_t}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)$$ 接着求输出门ot内部的梯度，由于ot门计算公式为： $$o_t&#x3D;\\sigma(x_t\\times W_{ox}+h_{t-1}\\times W_{oh}+b_o)$$ 所以求ot门内部的梯度，即可得到bo的梯度，由于sigmoid的导数可以表示为： $$S^{‘}(x)&#x3D;S(x)(1-S(x))$$ 所以，bo的梯度为： $$\\frac{\\partial L}{\\partial b_o}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))$$ 这里先提一下矩阵乘法的梯度计算公式： $$Y&#x3D;X\\times W$$ $$\\frac{\\partial L}{\\partial X}&#x3D;\\frac{\\partial L}{\\partial Y}\\times W^T$$ $$\\frac{\\partial L}{\\partial W}&#x3D;X^T\\times\\frac{\\partial L}{\\partial Y}$$ 由此可以进一步计算出$x_t、W_{ox}、h_{t-1}、W_{oh}$的梯度分别为： $$\\frac{\\partial L}{\\partial h_{t-1}}&#x3D;\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))\\right]\\times W_{oh}^T$$ $$\\frac{\\partial L}{\\partial x_t}&#x3D;\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot(o_t\\cdot(1-o_t))\\right]\\times W_{ox}^T$$ $$\\frac{\\partial L}{\\partial W_{oh}}&#x3D;h_{t-1}^T\\times\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot\\left(o_t\\cdot(1-o_t)\\right)\\right]$$ $$\\frac{\\partial L}{\\partial W_{ox}}&#x3D;x_t^T\\times\\left[\\frac{\\partial L}{\\partial h_t}\\cdot\\tanh(C_t)\\cdot\\left(o_t\\cdot(1-o_t)\\right)\\right]$$ 3.1.2 其它门的反向传播由公式（1）可知，tanh(Ct)的梯度为ht的梯度与输出门ot的点积，根据tanh函数的梯度（$(\\tanh x)’&#x3D;1-\\tanh^2x$）可以得到Ct的梯度为： $$\\frac{\\partial L}{\\partial C_t}&#x3D;\\frac{\\partial L}{\\partial h_t}\\cdot o_t\\cdot(1-tanh^2(C_t)) \\tag{2}$$ 由于Ct计算公式为： $$C_t&#x3D;f_t\\cdot C_{t-1}+i_t\\cdot \\stackrel{\\sim }{C_t}$$ 所以可以得到$f_t、C_{t-1}、i_t、\\stackrel{\\sim }{C_t}$的梯度： $$\\frac{\\partial L}{\\partial f_t}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot C_{t-1}$$ $$\\frac{\\partial L}{\\partial C_{t-1}}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot f_t \\tag{3}$$ $$\\frac{\\partial L}{\\partial i_t}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot \\stackrel{\\sim }{C_t}$$ $$\\frac{\\partial L}{\\partial \\stackrel{\\sim }{C_t}}&#x3D;\\frac{\\partial L}{\\partial C_t}\\cdot i_t$$ 现在得到了输入门、遗忘门、候补细胞状态的梯度后，可以按照和输出门一样的方法，先计算激活函数内部的梯度，再根据矩阵乘法的梯度公式进一步计算出$W、h_{t-1}、x_t$的梯度，这里不再赘述。 由于$h_{t-1}和x_t$在前向传播中被使用了4次，因此这里也会收到来自各个门和候补细胞状态的4个梯度，将其相加即可得到总梯度。 3.2 其它LSTM单元的反向传播其它LSTM单元来说，其与最后一个LSTM单元的区别在于ht和Ct的梯度来源不同。 前面提到，最后一个LSTM单元的ht只来自输出，而在3.1.2的最后，我们计算出了最后一个LSTM单元对$h_{t-1}$的梯度，因此倒数第二个LSTM单元则需要加上该梯度。此外，其输出方向也可能有梯度传来，比如多层LSTM的情况： 此时，下层LSTM每一个单元的输出，是上层每一个LSTM单元的输入。因此上层LSTM单元计算完$x_t$的梯度后，将会传回给下层LSTM单元。 同样的，Ct的梯度也来自两个方向，一方面来自当前LSTM单元中ht传来的梯度，如公式(2)；另一方面来自下一个LSTM单元，如公式(3)。 3.3 权重的更新前面说过，每一个LSTM单元都使用相同的权重，按照链式法则，这些权重也要接收来自每一个单元传来的梯度。 前面已经介绍了LSTM单元如何计算各个门和细胞状态的权重($W_h、W_x$以及$b$)的梯度，但是此时并不能更新这些权重，因为在前面的单元中，计算$h_{t-1}$和$x_t$的梯度时，需要用到这些权重，如果在后面的LSTM单元中修改了权重，那么前面的单元就无法得知原先的权重是多少。正确的做法是将每个单元计算出的权重的梯度累计起来，最后再更新权重参数。 4 LSTM反向传播的C++实现为了简单易懂，这里只用最基础的数组方式去实现。 4.1 矩阵乘法等基本函数实现矩阵乘法实现，经典的三个for循环，使用template指定输入输出数组的大小从而实现各种大小的矩阵相乘（后面全部函数都采用这种template的方法）： 矩阵乘法实现1234567891011template&lt;unsigned char row1,unsigned char col1,unsigned char col2&gt;void matrix_multiply(float inputarr1[row1][col1],float inputarr2[col1][col2],float outputarr[row1][col2]) &#123;\tfor(int i=0;i&lt;row1;i++)&#123; for(int j=0;j&lt;col2;j++)&#123; float temp=0.0; for(int k=0;k&lt;col1;k++) temp += inputarr1[i][k] * inputarr2[k][j]; outputarr[i][j] = temp; &#125;\t&#125;&#125; $A^T\\times B$的实现，可以理解为第一个矩阵的某一列与第二个矩阵的每一列相乘，作为结果的某一行的每一列数据，结果的行个数与输入的列个数一致： 图片演示 代码实现123456789101112template&lt;unsigned char row1,unsigned char col1,unsigned char col2&gt;void matrix_transpose1_multiply(float inputarr1[row1][col1],float inputarr2[row1][col2],float outputarr[col1][col2])&#123;\tfor(int i=0;i&lt;col1;i++)&#123; for(int j=0;j&lt;col2;j++)&#123; float temp = 0.0; for(int k=0;k&lt;row1;k++) temp += inputarr1[k][i] * inputarr2[k][j]; outputarr[i][j] = temp; &#125;\t&#125;&#125; $A\\times B^T$的实现，可以理解为第一个矩阵的某一行与第二个矩阵的每一行相乘，作为结果的某一行的每一列数据，结果的行个数与输入的行个数一致： 图片演示 代码实现123456789101112template&lt;unsigned char row1,unsigned char col1,unsigned char row2&gt;void matrix_transpose2_multiply(float inputarr1[row1][col1],float inputarr2[row2][col1],float outputarr[row1][row2])&#123;\tfor(int i=0;i&lt;row1;i++)&#123; for(int j=0;j&lt;row2;j++)&#123; float temp = 0.0; for(int k=0;k&lt;col1;k++) temp += inputarr1[i][k] * inputarr2[j][k]; outputarr[i][j] = temp; &#125;\t&#125;&#125; 4.2 反向传播的实现代码4.2.1 计算门和候补细胞状态的梯度由于计算各个门和候补细胞状态梯度的过程是类似的，因此将其封装为一个函数： 计算门和候补细胞状态的梯度1234567891011121314151617181920212223242526272829303132333435363738template&lt;int input_dims,int output_dims&gt;void compute_doors_gredient(float delta[1][output_dims],float K[1][output_dims],float gate[1][output_dims],\tfloat h_weight[output_dims][output_dims],float x_weight[input_dims][output_dims],\tfloat h_input[1][output_dims],float x_input[1][input_dims],\tfloat ht_delta[1][output_dims],float input_delta[1][input_dims],\tfloat delta_wx[input_dims][output_dims],float delta_wh[output_dims][output_dims],float delta_wb[output_dims],\tint choice=0)&#123;\tfloat delta_gate[1][output_dims];\tdot_product&lt;1,output_dims&gt;(delta,K,delta_gate); float delta_inner[1][output_dims];\tif(choice==0) sigmoid_delta&lt;1,output_dims&gt;(delta_gate,gate,delta_inner);\telse tanh_delta&lt;1,output_dims&gt;(delta_gate,gate,delta_inner);\tfloat temp_delta_wx[input_dims][output_dims];\tfloat temp_delta_wh[output_dims][output_dims]; matrix_transpose2_multiply&lt;1,output_dims,output_dims&gt;(delta_inner,h_weight,ht_delta);\tmatrix_transpose2_multiply&lt;1,output_dims,input_dims&gt;(delta_inner,x_weight,input_delta);\tmatrix_transpose1_multiply&lt;1,input_dims,output_dims&gt;(x_input,delta_inner,temp_delta_wx);\tmatrix_transpose1_multiply&lt;1,output_dims,output_dims&gt;(h_input,delta_inner,temp_delta_wh); for(int i=0;i&lt;output_dims;i++)&#123; delta_wb[i] += delta_inner[0][i];\t&#125;\tfor(int i=0;i&lt;input_dims;i++) for(int j=0;j&lt;output_dims;j++) delta_wx[i][j] += temp_delta_wx[i][j]; for(int i=0;i&lt;output_dims;i++) for(int j=0;j&lt;output_dims;j++) delta_wh[i][j] += temp_delta_wh[i][j];&#125; 在计算输出门时，delta为$\\frac{\\partial L}{\\partial h_t}$，其余均为$\\frac{\\partial L}{\\partial C_t}$。 K对于输出门来说是$tanh(C_t)$，对于输入门来说是$\\stackrel{\\sim }{C_t}$，依此类推，忘记的可以回头看前面公式，都有一个点积的操作，而后才是求激活函数的梯度，最后求各个权重和输入的梯度。部分函数并未给出，写起来也很简单，比如点积就是矩阵对应位置相乘。 这里权重的梯度使用“+&#x3D;”也是前面提到的，权重的梯度需要先累加起来。 4.2.2 LSTM反向传播的主函数LSTM反向传播主函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123template&lt;int timesteps,int input_dims,int output_dims&gt;void lstm_backpropagation(float learning_rate,float info[timesteps][6*output_dims],float delta[timesteps][output_dims],\tfloat input[timesteps][input_dims],float cells_output[timesteps][output_dims],float first_ht[1][output_dims],\tfloat wx_i[input_dims][output_dims],float wh_i[output_dims][output_dims],float b_i[output_dims],\tfloat wx_f[input_dims][output_dims],float wh_f[output_dims][output_dims],float b_f[output_dims],\tfloat wx_c[input_dims][output_dims],float wh_c[output_dims][output_dims],float b_c[output_dims],\tfloat wx_o[input_dims][output_dims],float wh_o[output_dims][output_dims],float b_o[output_dims],\tfloat delta_input[timesteps][input_dims])&#123;\tfloat delta_ct[1][output_dims];\tfloat delta_ht[1][output_dims]; //save total loss\tfloat delta_wx_i[input_dims][output_dims];float delta_wh_i[output_dims][output_dims];float delta_b_i[output_dims];\tfloat delta_wx_f[input_dims][output_dims];float delta_wh_f[output_dims][output_dims];float delta_b_f[output_dims];\tfloat delta_wx_c[input_dims][output_dims];float delta_wh_c[output_dims][output_dims];float delta_b_c[output_dims];\tfloat delta_wx_o[input_dims][output_dims];float delta_wh_o[output_dims][output_dims];float delta_b_o[output_dims]; //initial\tfor(int i=0;i&lt;input_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; delta_wx_i[i][j] = 0;\tdelta_wx_f[i][j] = 0;\tdelta_wx_c[i][j] = 0;\tdelta_wx_o[i][j] = 0; &#125;\t&#125; for(int i=0;i&lt;output_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; delta_wh_i[i][j] = 0;\tdelta_wh_f[i][j] = 0;\tdelta_wh_c[i][j] = 0;\tdelta_wh_o[i][j] = 0; &#125; delta_b_i[i] = 0;\tdelta_b_f[i] = 0;\tdelta_b_c[i] = 0;\tdelta_b_o[i] = 0; delta_ht[0][i] = 0;\tdelta_ct[0][i] = 0;\t&#125; for(int i=timesteps-1;i&gt;=0;i--)\t&#123; float old_ct[1][output_dims]; float input_gate[1][output_dims]; float forget_gate[1][output_dims]; float prepared_cell[1][output_dims]; float output_gate[1][output_dims]; float tanc_t[1][output_dims]; float input_x[1][input_dims]; float input_h[1][output_dims]; //recover information for(int j=0;j&lt;output_dims;j++)&#123; old_ct[0][j] = info[i][j]; input_gate[0][j] = info[i][output_dims+j]; forget_gate[0][j] = info[i][2*output_dims+j]; prepared_cell[0][j] = info[i][3*output_dims+j]; output_gate[0][j] = info[i][4*output_dims+j]; tanc_t[0][j] = info[i][5*output_dims+j]; if(i!=0) input_h[0][j] = cells_output[i-1][j]; // old ht else input_h[0][j] = first_ht[0][j]; &#125; for(int j=0;j&lt;input_dims;j++) input_x[0][j] = input[i][j]; //the final cell&#x27;s del_ht only from delta //other&#x27;s delta_ht from latter del_ht and delta for(int j=0;j&lt;output_dims;j++)&#123; delta_ht[0][j] += delta[i][j]; &#125; //the final cell&#x27;s del_ct only from ht //other&#x27;s delta_ct from latter_cell&#x27;s del_ct and current del_ht float temp_delta_ct[1][output_dims]; dot_product&lt;1,output_dims&gt;(delta_ht,output_gate,temp_delta_ct); tanh_delta&lt;1,output_dims&gt;(temp_delta_ct,tanc_t,temp_delta_ct); for(int j=0;j&lt;output_dims;j++)&#123; delta_ct[0][j] += temp_delta_ct[0][j]; &#125; float delta_ht_i[1][output_dims]; float delta_ht_f[1][output_dims]; float delta_ht_c[1][output_dims]; float delta_ht_o[1][output_dims]; float delta_input_i[1][input_dims]; float delta_input_f[1][input_dims]; float delta_input_c[1][input_dims]; float delta_input_o[1][input_dims]; compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,prepared_cell,input_gate, wh_i,wx_i,input_h,input_x,delta_ht_i,delta_input_i,delta_wx_i,delta_wh_i,delta_b_i); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,old_ct, forget_gate, wh_f,wx_f,input_h,input_x,delta_ht_f,delta_input_f,delta_wx_f,delta_wh_f,delta_b_f); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ct,input_gate, prepared_cell,wh_c,wx_c,input_h,input_x,delta_ht_c,delta_input_c,delta_wx_c,delta_wh_c,delta_b_c,1); compute_doors_gredient&lt;input_dims,output_dims&gt;(delta_ht,tanc_t, output_gate, wh_o,wx_o,input_h,input_x,delta_ht_o,delta_input_o,delta_wx_o,delta_wh_o,delta_b_o); //compute delta_ht for previous cell for(int j=0;j&lt;output_dims;j++) delta_ht[0][j] = delta_ht_i[0][j]+delta_ht_f[0][j]+delta_ht_c[0][j]+delta_ht_o[0][j]; //compute delta_input(if use multi-layer lstm) for(int j=0;j&lt;input_dims;j++) delta_input[i][j] = delta_input_i[0][j]+delta_input_f[0][j]+delta_input_c[0][j]+delta_input_o[0][j]; //compute delta_ct for previous cell dot_product&lt;1,output_dims&gt;(delta_ct,forget_gate,delta_ct);\t&#125; //update weight\tfor(int i=0;i&lt;input_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; wx_i[i][j] -= learning_rate*delta_wx_i[i][j]; wx_f[i][j] -= learning_rate*delta_wx_f[i][j]; wx_c[i][j] -= learning_rate*delta_wx_c[i][j]; wx_o[i][j] -= learning_rate*delta_wx_o[i][j]; &#125;\t&#125; for(int i=0;i&lt;output_dims;i++)&#123; for(int j=0;j&lt;output_dims;j++)&#123; wh_i[i][j] -= learning_rate*delta_wh_i[i][j]; wh_f[i][j] -= learning_rate*delta_wh_f[i][j]; wh_c[i][j] -= learning_rate*delta_wh_c[i][j]; wh_o[i][j] -= learning_rate*delta_wh_o[i][j]; &#125; b_i[i] -= learning_rate*delta_b_i[i]; b_f[i] -= learning_rate*delta_b_f[i]; b_c[i] -= learning_rate*delta_b_c[i]; b_o[i] -= learning_rate*delta_b_o[i];\t&#125;&#125; “for(int i&#x3D;timesteps-1;i&gt;&#x3D;0;i- -)”即是从最后一个LSTM单元开始计算梯度： 首先需要还原前向传播计算的信息，比如前面说的计算输出门ot的梯度时，需要知道$tanh(C_t)$的值，计算ot门内部梯度时，也需要知道ot的值，其余同理。对于$h_{t-1}$，我在前向传播时将每一个LSTM单元的输出保存在cells_output中，因此代码中“i-1”代表上一次LSTM单元的输出。而对于第一个LSTM单元，如果ht和Ct采用零初始化，那么这里$h_{t-1}$就为0，但是Stateful LSTM等情况时，会出现不为0的情况，因此这里直接增加了一个参数，将最开始的ht传进来。 参数delta是每一个LSTM单元的输出方向传来的梯度，如果采用多层LSTM，那么就是下一层LSTM的delta_input。如果是LSTM接全连接这种，就只有最后一个LSTM单元的输出有梯度，其余为0。 接下来和公式推导过程一样，先计算ht的梯度，再计算Ct的梯度，最后计算权重和输入的梯度。delta_ht和delta_ct是后一个LSTM单元传给前一个单元的ht和Ct的梯度。本来需要判断是否是最后一个LSTM单元，如果是最后一个，那么delta_ht就等于输出方向传来的梯度，delta_ct就等于ht方向计算的梯度。其余的LSTM单元则是”+&#x3D;”，因为梯度来自两方面。但由于delta_ht和delta_ct本身就是零初始化的，所以全部采用”+&#x3D;”也没问题，还省去了判断过程。 在for循环结束后，也就是所有LSTM单元计算完权重的梯度后，再更新权重。 5 写在最后注意权重初始化问题，当初我在实现的过程中，不太关注初始化，导致梯度消失很快。 第一次写博客，有问题欢迎在评论区提出","tags":["LSTM反向传播"],"categories":["深度学习"]},{"path":"/css/mouse.css","content":"/*鼠标样式*/ body, html { cursor: url(/cur/pointer.cur), auto !important; } /* a, */ img { cursor: url(/cur/link.cur), auto !important; } summary { cursor: url(/cur/link.cur), /* !important可能导致bug */ auto !important; } p { cursor: url(/cur/text.cur), auto; } details { cursor: url(/cur/text.cur), auto; } /*a标签*/ a:hover { cursor: url(/cur/link.cur), auto; } input:hover{ cursor: url(/cur/text.cur), auto; } /*按钮*/ button:hover { cursor: url(/cur/link.cur), auto; } /*i标签*/ i:hover { cursor: url(/cur/link.cur), auto; } /*页脚a标签*/ #footer-wrap a:hover { cursor: url(/cur/link.cur), auto; } /*分页器*/ #pagination .page-number:hover { cursor: url(/cur/link.cur), auto; } /*头部的导航栏*/ #nav .site-page:hover { cursor: url(/cur/link.cur), auto; } /*鼠标样式END*/"}]